{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet + MNIST\n",
    "\n",
    "http://yann.lecun.com/exdb/lenet/\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:47.493.164 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:47.493.164 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:47.493.164 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:47.493.164 [mindspore\\dataset\\core\\validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:47.493.164 [mindspore\\dataset\\core\\validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.3020830154418945\n",
      "epoch: 1 step: 2, loss is 2.297511100769043\n",
      "epoch: 1 step: 3, loss is 2.3021178245544434\n",
      "epoch: 1 step: 4, loss is 2.2974588871002197\n",
      "epoch: 1 step: 5, loss is 2.2943406105041504\n",
      "epoch: 1 step: 6, loss is 2.306159734725952\n",
      "epoch: 1 step: 7, loss is 2.3057701587677\n",
      "epoch: 1 step: 8, loss is 2.3064053058624268\n",
      "epoch: 1 step: 9, loss is 2.3006913661956787\n",
      "epoch: 1 step: 10, loss is 2.312077522277832\n",
      "epoch: 1 step: 11, loss is 2.296149969100952\n",
      "epoch: 1 step: 12, loss is 2.3007116317749023\n",
      "epoch: 1 step: 13, loss is 2.3053767681121826\n",
      "epoch: 1 step: 14, loss is 2.3007004261016846\n",
      "epoch: 1 step: 15, loss is 2.304736614227295\n",
      "epoch: 1 step: 16, loss is 2.3093299865722656\n",
      "epoch: 1 step: 17, loss is 2.2995779514312744\n",
      "epoch: 1 step: 18, loss is 2.2984941005706787\n",
      "epoch: 1 step: 19, loss is 2.293814182281494\n",
      "epoch: 1 step: 20, loss is 2.3049755096435547\n",
      "epoch: 1 step: 21, loss is 2.300083875656128\n",
      "epoch: 1 step: 22, loss is 2.3048899173736572\n",
      "epoch: 1 step: 23, loss is 2.306323766708374\n",
      "epoch: 1 step: 24, loss is 2.310789108276367\n",
      "epoch: 1 step: 25, loss is 2.300226926803589\n",
      "epoch: 1 step: 26, loss is 2.3022525310516357\n",
      "epoch: 1 step: 27, loss is 2.294240713119507\n",
      "epoch: 1 step: 28, loss is 2.30753755569458\n",
      "epoch: 1 step: 29, loss is 2.2929251194000244\n",
      "epoch: 1 step: 30, loss is 2.299205780029297\n",
      "epoch: 1 step: 31, loss is 2.2999651432037354\n",
      "epoch: 1 step: 32, loss is 2.313391923904419\n",
      "epoch: 1 step: 33, loss is 2.3003790378570557\n",
      "epoch: 1 step: 34, loss is 2.299391031265259\n",
      "epoch: 1 step: 35, loss is 2.3012120723724365\n",
      "epoch: 1 step: 36, loss is 2.305823564529419\n",
      "epoch: 1 step: 37, loss is 2.302621841430664\n",
      "epoch: 1 step: 38, loss is 2.300354480743408\n",
      "epoch: 1 step: 39, loss is 2.295558214187622\n",
      "epoch: 1 step: 40, loss is 2.310014486312866\n",
      "epoch: 1 step: 41, loss is 2.3002049922943115\n",
      "epoch: 1 step: 42, loss is 2.2983155250549316\n",
      "epoch: 1 step: 43, loss is 2.2968571186065674\n",
      "epoch: 1 step: 44, loss is 2.295323371887207\n",
      "epoch: 1 step: 45, loss is 2.2970497608184814\n",
      "epoch: 1 step: 46, loss is 2.3070948123931885\n",
      "epoch: 1 step: 47, loss is 2.2906906604766846\n",
      "epoch: 1 step: 48, loss is 2.3212990760803223\n",
      "epoch: 1 step: 49, loss is 2.304253578186035\n",
      "epoch: 1 step: 50, loss is 2.3064370155334473\n",
      "epoch: 1 step: 51, loss is 2.297060489654541\n",
      "epoch: 1 step: 52, loss is 2.30519437789917\n",
      "epoch: 1 step: 53, loss is 2.3075811862945557\n",
      "epoch: 1 step: 54, loss is 2.302661657333374\n",
      "epoch: 1 step: 55, loss is 2.2898051738739014\n",
      "epoch: 1 step: 56, loss is 2.300403594970703\n",
      "epoch: 1 step: 57, loss is 2.3049590587615967\n",
      "epoch: 1 step: 58, loss is 2.3026585578918457\n",
      "epoch: 1 step: 59, loss is 2.2902684211730957\n",
      "epoch: 1 step: 60, loss is 2.299154043197632\n",
      "epoch: 1 step: 61, loss is 2.317760944366455\n",
      "epoch: 1 step: 62, loss is 2.303215980529785\n",
      "epoch: 1 step: 63, loss is 2.3311996459960938\n",
      "epoch: 1 step: 64, loss is 2.3033835887908936\n",
      "epoch: 1 step: 65, loss is 2.302539110183716\n",
      "epoch: 1 step: 66, loss is 2.2951016426086426\n",
      "epoch: 1 step: 67, loss is 2.299901247024536\n",
      "epoch: 1 step: 68, loss is 2.3013527393341064\n",
      "epoch: 1 step: 69, loss is 2.307292938232422\n",
      "epoch: 1 step: 70, loss is 2.2909748554229736\n",
      "epoch: 1 step: 71, loss is 2.302431106567383\n",
      "epoch: 1 step: 72, loss is 2.29130220413208\n",
      "epoch: 1 step: 73, loss is 2.314220666885376\n",
      "epoch: 1 step: 74, loss is 2.2813527584075928\n",
      "epoch: 1 step: 75, loss is 2.309666872024536\n",
      "epoch: 1 step: 76, loss is 2.293121814727783\n",
      "epoch: 1 step: 77, loss is 2.2927768230438232\n",
      "epoch: 1 step: 78, loss is 2.3017501831054688\n",
      "epoch: 1 step: 79, loss is 2.303302526473999\n",
      "epoch: 1 step: 80, loss is 2.300657033920288\n",
      "epoch: 1 step: 81, loss is 2.323739767074585\n",
      "epoch: 1 step: 82, loss is 2.300440788269043\n",
      "epoch: 1 step: 83, loss is 2.3098719120025635\n",
      "epoch: 1 step: 84, loss is 2.3092563152313232\n",
      "epoch: 1 step: 85, loss is 2.313765048980713\n",
      "epoch: 1 step: 86, loss is 2.313493013381958\n",
      "epoch: 1 step: 87, loss is 2.2963337898254395\n",
      "epoch: 1 step: 88, loss is 2.312152147293091\n",
      "epoch: 1 step: 89, loss is 2.3045008182525635\n",
      "epoch: 1 step: 90, loss is 2.3204333782196045\n",
      "epoch: 1 step: 91, loss is 2.2947802543640137\n",
      "epoch: 1 step: 92, loss is 2.296168088912964\n",
      "epoch: 1 step: 93, loss is 2.2969865798950195\n",
      "epoch: 1 step: 94, loss is 2.292175054550171\n",
      "epoch: 1 step: 95, loss is 2.303164005279541\n",
      "epoch: 1 step: 96, loss is 2.3113386631011963\n",
      "epoch: 1 step: 97, loss is 2.3122804164886475\n",
      "epoch: 1 step: 98, loss is 2.3263370990753174\n",
      "epoch: 1 step: 99, loss is 2.2905001640319824\n",
      "epoch: 1 step: 100, loss is 2.308671474456787\n",
      "epoch: 1 step: 101, loss is 2.2801883220672607\n",
      "epoch: 1 step: 102, loss is 2.301889419555664\n",
      "epoch: 1 step: 103, loss is 2.3054206371307373\n",
      "epoch: 1 step: 104, loss is 2.2831239700317383\n",
      "epoch: 1 step: 105, loss is 2.307374954223633\n",
      "epoch: 1 step: 106, loss is 2.2937936782836914\n",
      "epoch: 1 step: 107, loss is 2.3033673763275146\n",
      "epoch: 1 step: 108, loss is 2.3027923107147217\n",
      "epoch: 1 step: 109, loss is 2.307419538497925\n",
      "epoch: 1 step: 110, loss is 2.3086307048797607\n",
      "epoch: 1 step: 111, loss is 2.299490451812744\n",
      "epoch: 1 step: 112, loss is 2.2931127548217773\n",
      "epoch: 1 step: 113, loss is 2.3054587841033936\n",
      "epoch: 1 step: 114, loss is 2.284979820251465\n",
      "epoch: 1 step: 115, loss is 2.29994535446167\n",
      "epoch: 1 step: 116, loss is 2.2748799324035645\n",
      "epoch: 1 step: 117, loss is 2.3065812587738037\n",
      "epoch: 1 step: 118, loss is 2.295694589614868\n",
      "epoch: 1 step: 119, loss is 2.3157665729522705\n",
      "epoch: 1 step: 120, loss is 2.306384563446045\n",
      "epoch: 1 step: 121, loss is 2.315777540206909\n",
      "epoch: 1 step: 122, loss is 2.3092198371887207\n",
      "epoch: 1 step: 123, loss is 2.2937042713165283\n",
      "epoch: 1 step: 124, loss is 2.3055739402770996\n",
      "epoch: 1 step: 125, loss is 2.3090310096740723\n",
      "epoch: 1 step: 126, loss is 2.3145687580108643\n",
      "epoch: 1 step: 127, loss is 2.2829253673553467\n",
      "epoch: 1 step: 128, loss is 2.2952258586883545\n",
      "epoch: 1 step: 129, loss is 2.310028076171875\n",
      "epoch: 1 step: 130, loss is 2.3084466457366943\n",
      "epoch: 1 step: 131, loss is 2.300173759460449\n",
      "epoch: 1 step: 132, loss is 2.3110342025756836\n",
      "epoch: 1 step: 133, loss is 2.293134927749634\n",
      "epoch: 1 step: 134, loss is 2.3043558597564697\n",
      "epoch: 1 step: 135, loss is 2.288649797439575\n",
      "epoch: 1 step: 136, loss is 2.300729990005493\n",
      "epoch: 1 step: 137, loss is 2.282128095626831\n",
      "epoch: 1 step: 138, loss is 2.290262460708618\n",
      "epoch: 1 step: 139, loss is 2.294694185256958\n",
      "epoch: 1 step: 140, loss is 2.3026552200317383\n",
      "epoch: 1 step: 141, loss is 2.306962490081787\n",
      "epoch: 1 step: 142, loss is 2.278386116027832\n",
      "epoch: 1 step: 143, loss is 2.2955832481384277\n",
      "epoch: 1 step: 144, loss is 2.3012428283691406\n",
      "epoch: 1 step: 145, loss is 2.3080995082855225\n",
      "epoch: 1 step: 146, loss is 2.2933452129364014\n",
      "epoch: 1 step: 147, loss is 2.300206422805786\n",
      "epoch: 1 step: 148, loss is 2.297360420227051\n",
      "epoch: 1 step: 149, loss is 2.3047313690185547\n",
      "epoch: 1 step: 150, loss is 2.2878518104553223\n",
      "epoch: 1 step: 151, loss is 2.312751531600952\n",
      "epoch: 1 step: 152, loss is 2.2718465328216553\n",
      "epoch: 1 step: 153, loss is 2.2969958782196045\n",
      "epoch: 1 step: 154, loss is 2.321514844894409\n",
      "epoch: 1 step: 155, loss is 2.299478530883789\n",
      "epoch: 1 step: 156, loss is 2.283918857574463\n",
      "epoch: 1 step: 157, loss is 2.279238224029541\n",
      "epoch: 1 step: 158, loss is 2.306208372116089\n",
      "epoch: 1 step: 159, loss is 2.3118069171905518\n",
      "epoch: 1 step: 160, loss is 2.2683839797973633\n",
      "epoch: 1 step: 161, loss is 2.3188109397888184\n",
      "epoch: 1 step: 162, loss is 2.3087785243988037\n",
      "epoch: 1 step: 163, loss is 2.28714656829834\n",
      "epoch: 1 step: 164, loss is 2.305490493774414\n",
      "epoch: 1 step: 165, loss is 2.287356376647949\n",
      "epoch: 1 step: 166, loss is 2.3052096366882324\n",
      "epoch: 1 step: 167, loss is 2.2987759113311768\n",
      "epoch: 1 step: 168, loss is 2.30771541595459\n",
      "epoch: 1 step: 169, loss is 2.298232316970825\n",
      "epoch: 1 step: 170, loss is 2.2762880325317383\n",
      "epoch: 1 step: 171, loss is 2.309379816055298\n",
      "epoch: 1 step: 172, loss is 2.281287431716919\n",
      "epoch: 1 step: 173, loss is 2.271897077560425\n",
      "epoch: 1 step: 174, loss is 2.289822816848755\n",
      "epoch: 1 step: 175, loss is 2.308986186981201\n",
      "epoch: 1 step: 176, loss is 2.2880542278289795\n",
      "epoch: 1 step: 177, loss is 2.291015625\n",
      "epoch: 1 step: 178, loss is 2.301403522491455\n",
      "epoch: 1 step: 179, loss is 2.299567937850952\n",
      "epoch: 1 step: 180, loss is 2.3168840408325195\n",
      "epoch: 1 step: 181, loss is 2.2918694019317627\n",
      "epoch: 1 step: 182, loss is 2.300267457962036\n",
      "epoch: 1 step: 183, loss is 2.2852981090545654\n",
      "epoch: 1 step: 184, loss is 2.3317277431488037\n",
      "epoch: 1 step: 185, loss is 2.315861940383911\n",
      "epoch: 1 step: 186, loss is 2.3154149055480957\n",
      "epoch: 1 step: 187, loss is 2.302710771560669\n",
      "epoch: 1 step: 188, loss is 2.2836952209472656\n",
      "epoch: 1 step: 189, loss is 2.3168129920959473\n",
      "epoch: 1 step: 190, loss is 2.3243300914764404\n",
      "epoch: 1 step: 191, loss is 2.281310558319092\n",
      "epoch: 1 step: 192, loss is 2.320225954055786\n",
      "epoch: 1 step: 193, loss is 2.3063178062438965\n",
      "epoch: 1 step: 194, loss is 2.3061764240264893\n",
      "epoch: 1 step: 195, loss is 2.311782121658325\n",
      "epoch: 1 step: 196, loss is 2.309354782104492\n",
      "epoch: 1 step: 197, loss is 2.287140369415283\n",
      "epoch: 1 step: 198, loss is 2.2632694244384766\n",
      "epoch: 1 step: 199, loss is 2.2924137115478516\n",
      "epoch: 1 step: 200, loss is 2.306492567062378\n",
      "epoch: 1 step: 201, loss is 2.300060272216797\n",
      "epoch: 1 step: 202, loss is 2.3098714351654053\n",
      "epoch: 1 step: 203, loss is 2.305272102355957\n",
      "epoch: 1 step: 204, loss is 2.3133842945098877\n",
      "epoch: 1 step: 205, loss is 2.299609422683716\n",
      "epoch: 1 step: 206, loss is 2.292019844055176\n",
      "epoch: 1 step: 207, loss is 2.285151243209839\n",
      "epoch: 1 step: 208, loss is 2.3029134273529053\n",
      "epoch: 1 step: 209, loss is 2.3085527420043945\n",
      "epoch: 1 step: 210, loss is 2.2970354557037354\n",
      "epoch: 1 step: 211, loss is 2.293639659881592\n",
      "epoch: 1 step: 212, loss is 2.2804436683654785\n",
      "epoch: 1 step: 213, loss is 2.2953975200653076\n",
      "epoch: 1 step: 214, loss is 2.3205673694610596\n",
      "epoch: 1 step: 215, loss is 2.3009274005889893\n",
      "epoch: 1 step: 216, loss is 2.300424337387085\n",
      "epoch: 1 step: 217, loss is 2.32108736038208\n",
      "epoch: 1 step: 218, loss is 2.2786519527435303\n",
      "epoch: 1 step: 219, loss is 2.325031280517578\n",
      "epoch: 1 step: 220, loss is 2.3202619552612305\n",
      "epoch: 1 step: 221, loss is 2.2922685146331787\n",
      "epoch: 1 step: 222, loss is 2.2987802028656006\n",
      "epoch: 1 step: 223, loss is 2.303570508956909\n",
      "epoch: 1 step: 224, loss is 2.2990660667419434\n",
      "epoch: 1 step: 225, loss is 2.277578592300415\n",
      "epoch: 1 step: 226, loss is 2.3118715286254883\n",
      "epoch: 1 step: 227, loss is 2.2998714447021484\n",
      "epoch: 1 step: 228, loss is 2.3219447135925293\n",
      "epoch: 1 step: 229, loss is 2.2977514266967773\n",
      "epoch: 1 step: 230, loss is 2.317363977432251\n",
      "epoch: 1 step: 231, loss is 2.3067288398742676\n",
      "epoch: 1 step: 232, loss is 2.305906295776367\n",
      "epoch: 1 step: 233, loss is 2.316667318344116\n",
      "epoch: 1 step: 234, loss is 2.3008921146392822\n",
      "epoch: 1 step: 235, loss is 2.29537296295166\n",
      "epoch: 1 step: 236, loss is 2.3272993564605713\n",
      "epoch: 1 step: 237, loss is 2.3158607482910156\n",
      "epoch: 1 step: 238, loss is 2.2985382080078125\n",
      "epoch: 1 step: 239, loss is 2.2908878326416016\n",
      "epoch: 1 step: 240, loss is 2.31166410446167\n",
      "epoch: 1 step: 241, loss is 2.2952780723571777\n",
      "epoch: 1 step: 242, loss is 2.3192126750946045\n",
      "epoch: 1 step: 243, loss is 2.309987783432007\n",
      "epoch: 1 step: 244, loss is 2.275020122528076\n",
      "epoch: 1 step: 245, loss is 2.314903497695923\n",
      "epoch: 1 step: 246, loss is 2.2948553562164307\n",
      "epoch: 1 step: 247, loss is 2.286363124847412\n",
      "epoch: 1 step: 248, loss is 2.3082306385040283\n",
      "epoch: 1 step: 249, loss is 2.3252151012420654\n",
      "epoch: 1 step: 250, loss is 2.3159875869750977\n",
      "epoch: 1 step: 251, loss is 2.266087055206299\n",
      "epoch: 1 step: 252, loss is 2.3211793899536133\n",
      "epoch: 1 step: 253, loss is 2.307596445083618\n",
      "epoch: 1 step: 254, loss is 2.300319194793701\n",
      "epoch: 1 step: 255, loss is 2.303971767425537\n",
      "epoch: 1 step: 256, loss is 2.2848410606384277\n",
      "epoch: 1 step: 257, loss is 2.3131141662597656\n",
      "epoch: 1 step: 258, loss is 2.2842469215393066\n",
      "epoch: 1 step: 259, loss is 2.327467918395996\n",
      "epoch: 1 step: 260, loss is 2.2995448112487793\n",
      "epoch: 1 step: 261, loss is 2.3268706798553467\n",
      "epoch: 1 step: 262, loss is 2.319897413253784\n",
      "epoch: 1 step: 263, loss is 2.3021068572998047\n",
      "epoch: 1 step: 264, loss is 2.3114137649536133\n",
      "epoch: 1 step: 265, loss is 2.3137123584747314\n",
      "epoch: 1 step: 266, loss is 2.290210008621216\n",
      "epoch: 1 step: 267, loss is 2.287877082824707\n",
      "epoch: 1 step: 268, loss is 2.3127355575561523\n",
      "epoch: 1 step: 269, loss is 2.286745071411133\n",
      "epoch: 1 step: 270, loss is 2.309809684753418\n",
      "epoch: 1 step: 271, loss is 2.315293312072754\n",
      "epoch: 1 step: 272, loss is 2.2908263206481934\n",
      "epoch: 1 step: 273, loss is 2.289785861968994\n",
      "epoch: 1 step: 274, loss is 2.2821736335754395\n",
      "epoch: 1 step: 275, loss is 2.319814920425415\n",
      "epoch: 1 step: 276, loss is 2.28702712059021\n",
      "epoch: 1 step: 277, loss is 2.2982943058013916\n",
      "epoch: 1 step: 278, loss is 2.3024396896362305\n",
      "epoch: 1 step: 279, loss is 2.280014753341675\n",
      "epoch: 1 step: 280, loss is 2.289883852005005\n",
      "epoch: 1 step: 281, loss is 2.3180952072143555\n",
      "epoch: 1 step: 282, loss is 2.3169033527374268\n",
      "epoch: 1 step: 283, loss is 2.2983715534210205\n",
      "epoch: 1 step: 284, loss is 2.296010971069336\n",
      "epoch: 1 step: 285, loss is 2.2852983474731445\n",
      "epoch: 1 step: 286, loss is 2.302618980407715\n",
      "epoch: 1 step: 287, loss is 2.2947230339050293\n",
      "epoch: 1 step: 288, loss is 2.286008358001709\n",
      "epoch: 1 step: 289, loss is 2.2968783378601074\n",
      "epoch: 1 step: 290, loss is 2.30361008644104\n",
      "epoch: 1 step: 291, loss is 2.298152446746826\n",
      "epoch: 1 step: 292, loss is 2.298973798751831\n",
      "epoch: 1 step: 293, loss is 2.3139541149139404\n",
      "epoch: 1 step: 294, loss is 2.3050246238708496\n",
      "epoch: 1 step: 295, loss is 2.3112213611602783\n",
      "epoch: 1 step: 296, loss is 2.3013641834259033\n",
      "epoch: 1 step: 297, loss is 2.306297779083252\n",
      "epoch: 1 step: 298, loss is 2.2874603271484375\n",
      "epoch: 1 step: 299, loss is 2.3127217292785645\n",
      "epoch: 1 step: 300, loss is 2.3120501041412354\n",
      "epoch: 1 step: 301, loss is 2.313852548599243\n",
      "epoch: 1 step: 302, loss is 2.313840627670288\n",
      "epoch: 1 step: 303, loss is 2.312520742416382\n",
      "epoch: 1 step: 304, loss is 2.287062883377075\n",
      "epoch: 1 step: 305, loss is 2.2623283863067627\n",
      "epoch: 1 step: 306, loss is 2.3107497692108154\n",
      "epoch: 1 step: 307, loss is 2.3093762397766113\n",
      "epoch: 1 step: 308, loss is 2.315290689468384\n",
      "epoch: 1 step: 309, loss is 2.3018369674682617\n",
      "epoch: 1 step: 310, loss is 2.301690101623535\n",
      "epoch: 1 step: 311, loss is 2.311732292175293\n",
      "epoch: 1 step: 312, loss is 2.295393943786621\n",
      "epoch: 1 step: 313, loss is 2.307046413421631\n",
      "epoch: 1 step: 314, loss is 2.3223347663879395\n",
      "epoch: 1 step: 315, loss is 2.2782576084136963\n",
      "epoch: 1 step: 316, loss is 2.283571481704712\n",
      "epoch: 1 step: 317, loss is 2.305976390838623\n",
      "epoch: 1 step: 318, loss is 2.296865463256836\n",
      "epoch: 1 step: 319, loss is 2.305861711502075\n",
      "epoch: 1 step: 320, loss is 2.3106799125671387\n",
      "epoch: 1 step: 321, loss is 2.292404890060425\n",
      "epoch: 1 step: 322, loss is 2.317406415939331\n",
      "epoch: 1 step: 323, loss is 2.2990055084228516\n",
      "epoch: 1 step: 324, loss is 2.292940139770508\n",
      "epoch: 1 step: 325, loss is 2.293947219848633\n",
      "epoch: 1 step: 326, loss is 2.3001344203948975\n",
      "epoch: 1 step: 327, loss is 2.308015823364258\n",
      "epoch: 1 step: 328, loss is 2.2649049758911133\n",
      "epoch: 1 step: 329, loss is 2.3051581382751465\n",
      "epoch: 1 step: 330, loss is 2.3141233921051025\n",
      "epoch: 1 step: 331, loss is 2.3092617988586426\n",
      "epoch: 1 step: 332, loss is 2.3109586238861084\n",
      "epoch: 1 step: 333, loss is 2.3112456798553467\n",
      "epoch: 1 step: 334, loss is 2.2989606857299805\n",
      "epoch: 1 step: 335, loss is 2.2767817974090576\n",
      "epoch: 1 step: 336, loss is 2.313689947128296\n",
      "epoch: 1 step: 337, loss is 2.293553352355957\n",
      "epoch: 1 step: 338, loss is 2.2954659461975098\n",
      "epoch: 1 step: 339, loss is 2.3050804138183594\n",
      "epoch: 1 step: 340, loss is 2.3161134719848633\n",
      "epoch: 1 step: 341, loss is 2.2915549278259277\n",
      "epoch: 1 step: 342, loss is 2.314631700515747\n",
      "epoch: 1 step: 343, loss is 2.2860217094421387\n",
      "epoch: 1 step: 344, loss is 2.3144216537475586\n",
      "epoch: 1 step: 345, loss is 2.2965216636657715\n",
      "epoch: 1 step: 346, loss is 2.297952175140381\n",
      "epoch: 1 step: 347, loss is 2.286159038543701\n",
      "epoch: 1 step: 348, loss is 2.290165424346924\n",
      "epoch: 1 step: 349, loss is 2.309035301208496\n",
      "epoch: 1 step: 350, loss is 2.3079288005828857\n",
      "epoch: 1 step: 351, loss is 2.3092451095581055\n",
      "epoch: 1 step: 352, loss is 2.3006391525268555\n",
      "epoch: 1 step: 353, loss is 2.322481155395508\n",
      "epoch: 1 step: 354, loss is 2.2962749004364014\n",
      "epoch: 1 step: 355, loss is 2.3129429817199707\n",
      "epoch: 1 step: 356, loss is 2.298133611679077\n",
      "epoch: 1 step: 357, loss is 2.2806036472320557\n",
      "epoch: 1 step: 358, loss is 2.2954323291778564\n",
      "epoch: 1 step: 359, loss is 2.3028640747070312\n",
      "epoch: 1 step: 360, loss is 2.3267440795898438\n",
      "epoch: 1 step: 361, loss is 2.3037776947021484\n",
      "epoch: 1 step: 362, loss is 2.309199333190918\n",
      "epoch: 1 step: 363, loss is 2.2964601516723633\n",
      "epoch: 1 step: 364, loss is 2.3061437606811523\n",
      "epoch: 1 step: 365, loss is 2.2926151752471924\n",
      "epoch: 1 step: 366, loss is 2.2994418144226074\n",
      "epoch: 1 step: 367, loss is 2.307568311691284\n",
      "epoch: 1 step: 368, loss is 2.2839102745056152\n",
      "epoch: 1 step: 369, loss is 2.3179402351379395\n",
      "epoch: 1 step: 370, loss is 2.2815051078796387\n",
      "epoch: 1 step: 371, loss is 2.2943994998931885\n",
      "epoch: 1 step: 372, loss is 2.3079798221588135\n",
      "epoch: 1 step: 373, loss is 2.303250789642334\n",
      "epoch: 1 step: 374, loss is 2.2997939586639404\n",
      "epoch: 1 step: 375, loss is 2.3174314498901367\n",
      "epoch: 1 step: 376, loss is 2.293562889099121\n",
      "epoch: 1 step: 377, loss is 2.296079635620117\n",
      "epoch: 1 step: 378, loss is 2.3157315254211426\n",
      "epoch: 1 step: 379, loss is 2.2957305908203125\n",
      "epoch: 1 step: 380, loss is 2.3111486434936523\n",
      "epoch: 1 step: 381, loss is 2.3101868629455566\n",
      "epoch: 1 step: 382, loss is 2.3114638328552246\n",
      "epoch: 1 step: 383, loss is 2.297071695327759\n",
      "epoch: 1 step: 384, loss is 2.309096097946167\n",
      "epoch: 1 step: 385, loss is 2.3085153102874756\n",
      "epoch: 1 step: 386, loss is 2.2947161197662354\n",
      "epoch: 1 step: 387, loss is 2.307668447494507\n",
      "epoch: 1 step: 388, loss is 2.294637441635132\n",
      "epoch: 1 step: 389, loss is 2.302244186401367\n",
      "epoch: 1 step: 390, loss is 2.306342363357544\n",
      "epoch: 1 step: 391, loss is 2.287583112716675\n",
      "epoch: 1 step: 392, loss is 2.309640884399414\n",
      "epoch: 1 step: 393, loss is 2.319157600402832\n",
      "epoch: 1 step: 394, loss is 2.302642822265625\n",
      "epoch: 1 step: 395, loss is 2.313624858856201\n",
      "epoch: 1 step: 396, loss is 2.3031177520751953\n",
      "epoch: 1 step: 397, loss is 2.309561014175415\n",
      "epoch: 1 step: 398, loss is 2.302830219268799\n",
      "epoch: 1 step: 399, loss is 2.3030619621276855\n",
      "epoch: 1 step: 400, loss is 2.2990798950195312\n",
      "epoch: 1 step: 401, loss is 2.2955806255340576\n",
      "epoch: 1 step: 402, loss is 2.3112435340881348\n",
      "epoch: 1 step: 403, loss is 2.2963171005249023\n",
      "epoch: 1 step: 404, loss is 2.306483507156372\n",
      "epoch: 1 step: 405, loss is 2.3061091899871826\n",
      "epoch: 1 step: 406, loss is 2.302931547164917\n",
      "epoch: 1 step: 407, loss is 2.3017146587371826\n",
      "epoch: 1 step: 408, loss is 2.2830326557159424\n",
      "epoch: 1 step: 409, loss is 2.3004953861236572\n",
      "epoch: 1 step: 410, loss is 2.2877678871154785\n",
      "epoch: 1 step: 411, loss is 2.288971185684204\n",
      "epoch: 1 step: 412, loss is 2.2905771732330322\n",
      "epoch: 1 step: 413, loss is 2.300966739654541\n",
      "epoch: 1 step: 414, loss is 2.297025680541992\n",
      "epoch: 1 step: 415, loss is 2.2967777252197266\n",
      "epoch: 1 step: 416, loss is 2.3050754070281982\n",
      "epoch: 1 step: 417, loss is 2.3006386756896973\n",
      "epoch: 1 step: 418, loss is 2.326878070831299\n",
      "epoch: 1 step: 419, loss is 2.298570156097412\n",
      "epoch: 1 step: 420, loss is 2.2926816940307617\n",
      "epoch: 1 step: 421, loss is 2.2864108085632324\n",
      "epoch: 1 step: 422, loss is 2.2989673614501953\n",
      "epoch: 1 step: 423, loss is 2.3063573837280273\n",
      "epoch: 1 step: 424, loss is 2.296893358230591\n",
      "epoch: 1 step: 425, loss is 2.3169243335723877\n",
      "epoch: 1 step: 426, loss is 2.300682306289673\n",
      "epoch: 1 step: 427, loss is 2.3038792610168457\n",
      "epoch: 1 step: 428, loss is 2.2866532802581787\n",
      "epoch: 1 step: 429, loss is 2.298089027404785\n",
      "epoch: 1 step: 430, loss is 2.2900965213775635\n",
      "epoch: 1 step: 431, loss is 2.283311605453491\n",
      "epoch: 1 step: 432, loss is 2.315892457962036\n",
      "epoch: 1 step: 433, loss is 2.3039698600769043\n",
      "epoch: 1 step: 434, loss is 2.310302495956421\n",
      "epoch: 1 step: 435, loss is 2.295921802520752\n",
      "epoch: 1 step: 436, loss is 2.306853771209717\n",
      "epoch: 1 step: 437, loss is 2.3191800117492676\n",
      "epoch: 1 step: 438, loss is 2.2986602783203125\n",
      "epoch: 1 step: 439, loss is 2.291508197784424\n",
      "epoch: 1 step: 440, loss is 2.3084843158721924\n",
      "epoch: 1 step: 441, loss is 2.2877602577209473\n",
      "epoch: 1 step: 442, loss is 2.2987663745880127\n",
      "epoch: 1 step: 443, loss is 2.301938772201538\n",
      "epoch: 1 step: 444, loss is 2.318655252456665\n",
      "epoch: 1 step: 445, loss is 2.3177847862243652\n",
      "epoch: 1 step: 446, loss is 2.2893989086151123\n",
      "epoch: 1 step: 447, loss is 2.3049910068511963\n",
      "epoch: 1 step: 448, loss is 2.2901840209960938\n",
      "epoch: 1 step: 449, loss is 2.29941987991333\n",
      "epoch: 1 step: 450, loss is 2.315478563308716\n",
      "epoch: 1 step: 451, loss is 2.3054723739624023\n",
      "epoch: 1 step: 452, loss is 2.298839807510376\n",
      "epoch: 1 step: 453, loss is 2.296304941177368\n",
      "epoch: 1 step: 454, loss is 2.2920610904693604\n",
      "epoch: 1 step: 455, loss is 2.2954115867614746\n",
      "epoch: 1 step: 456, loss is 2.316122531890869\n",
      "epoch: 1 step: 457, loss is 2.300426959991455\n",
      "epoch: 1 step: 458, loss is 2.277920961380005\n",
      "epoch: 1 step: 459, loss is 2.307332992553711\n",
      "epoch: 1 step: 460, loss is 2.289156675338745\n",
      "epoch: 1 step: 461, loss is 2.3036041259765625\n",
      "epoch: 1 step: 462, loss is 2.292264223098755\n",
      "epoch: 1 step: 463, loss is 2.3133647441864014\n",
      "epoch: 1 step: 464, loss is 2.318371534347534\n",
      "epoch: 1 step: 465, loss is 2.292250394821167\n",
      "epoch: 1 step: 466, loss is 2.309474229812622\n",
      "epoch: 1 step: 467, loss is 2.3068346977233887\n",
      "epoch: 1 step: 468, loss is 2.30214262008667\n",
      "epoch: 1 step: 469, loss is 2.2913575172424316\n",
      "epoch: 1 step: 470, loss is 2.307109832763672\n",
      "epoch: 1 step: 471, loss is 2.307121992111206\n",
      "epoch: 1 step: 472, loss is 2.3054866790771484\n",
      "epoch: 1 step: 473, loss is 2.2921032905578613\n",
      "epoch: 1 step: 474, loss is 2.301318883895874\n",
      "epoch: 1 step: 475, loss is 2.2969541549682617\n",
      "epoch: 1 step: 476, loss is 2.300791025161743\n",
      "epoch: 1 step: 477, loss is 2.302217721939087\n",
      "epoch: 1 step: 478, loss is 2.309523582458496\n",
      "epoch: 1 step: 479, loss is 2.2990307807922363\n",
      "epoch: 1 step: 480, loss is 2.3136513233184814\n",
      "epoch: 1 step: 481, loss is 2.2980594635009766\n",
      "epoch: 1 step: 482, loss is 2.296548843383789\n",
      "epoch: 1 step: 483, loss is 2.300539970397949\n",
      "epoch: 1 step: 484, loss is 2.298197031021118\n",
      "epoch: 1 step: 485, loss is 2.300902843475342\n",
      "epoch: 1 step: 486, loss is 2.300551414489746\n",
      "epoch: 1 step: 487, loss is 2.3072474002838135\n",
      "epoch: 1 step: 488, loss is 2.2961530685424805\n",
      "epoch: 1 step: 489, loss is 2.2870678901672363\n",
      "epoch: 1 step: 490, loss is 2.3062939643859863\n",
      "epoch: 1 step: 491, loss is 2.309767484664917\n",
      "epoch: 1 step: 492, loss is 2.298189163208008\n",
      "epoch: 1 step: 493, loss is 2.294877052307129\n",
      "epoch: 1 step: 494, loss is 2.2767975330352783\n",
      "epoch: 1 step: 495, loss is 2.280564785003662\n",
      "epoch: 1 step: 496, loss is 2.2904133796691895\n",
      "epoch: 1 step: 497, loss is 2.2728354930877686\n",
      "epoch: 1 step: 498, loss is 2.2870681285858154\n",
      "epoch: 1 step: 499, loss is 2.2895565032958984\n",
      "epoch: 1 step: 500, loss is 2.3017494678497314\n",
      "epoch: 1 step: 501, loss is 2.30084228515625\n",
      "epoch: 1 step: 502, loss is 2.296415090560913\n",
      "epoch: 1 step: 503, loss is 2.2984437942504883\n",
      "epoch: 1 step: 504, loss is 2.3004419803619385\n",
      "epoch: 1 step: 505, loss is 2.2771661281585693\n",
      "epoch: 1 step: 506, loss is 2.309612989425659\n",
      "epoch: 1 step: 507, loss is 2.3215317726135254\n",
      "epoch: 1 step: 508, loss is 2.2933669090270996\n",
      "epoch: 1 step: 509, loss is 2.3041696548461914\n",
      "epoch: 1 step: 510, loss is 2.31874418258667\n",
      "epoch: 1 step: 511, loss is 2.3232994079589844\n",
      "epoch: 1 step: 512, loss is 2.305889844894409\n",
      "epoch: 1 step: 513, loss is 2.3251473903656006\n",
      "epoch: 1 step: 514, loss is 2.3070056438446045\n",
      "epoch: 1 step: 515, loss is 2.3225791454315186\n",
      "epoch: 1 step: 516, loss is 2.325695514678955\n",
      "epoch: 1 step: 517, loss is 2.2873919010162354\n",
      "epoch: 1 step: 518, loss is 2.2915585041046143\n",
      "epoch: 1 step: 519, loss is 2.2940642833709717\n",
      "epoch: 1 step: 520, loss is 2.2856571674346924\n",
      "epoch: 1 step: 521, loss is 2.311765670776367\n",
      "epoch: 1 step: 522, loss is 2.2877042293548584\n",
      "epoch: 1 step: 523, loss is 2.308532238006592\n",
      "epoch: 1 step: 524, loss is 2.283815622329712\n",
      "epoch: 1 step: 525, loss is 2.295252561569214\n",
      "epoch: 1 step: 526, loss is 2.2902541160583496\n",
      "epoch: 1 step: 527, loss is 2.323634386062622\n",
      "epoch: 1 step: 528, loss is 2.3065381050109863\n",
      "epoch: 1 step: 529, loss is 2.2892255783081055\n",
      "epoch: 1 step: 530, loss is 2.314901113510132\n",
      "epoch: 1 step: 531, loss is 2.3031234741210938\n",
      "epoch: 1 step: 532, loss is 2.31194806098938\n",
      "epoch: 1 step: 533, loss is 2.288123369216919\n",
      "epoch: 1 step: 534, loss is 2.2861039638519287\n",
      "epoch: 1 step: 535, loss is 2.2963061332702637\n",
      "epoch: 1 step: 536, loss is 2.292022228240967\n",
      "epoch: 1 step: 537, loss is 2.283555269241333\n",
      "epoch: 1 step: 538, loss is 2.2911641597747803\n",
      "epoch: 1 step: 539, loss is 2.3002309799194336\n",
      "epoch: 1 step: 540, loss is 2.3097715377807617\n",
      "epoch: 1 step: 541, loss is 2.306020736694336\n",
      "epoch: 1 step: 542, loss is 2.294912338256836\n",
      "epoch: 1 step: 543, loss is 2.3063442707061768\n",
      "epoch: 1 step: 544, loss is 2.2951912879943848\n",
      "epoch: 1 step: 545, loss is 2.3027749061584473\n",
      "epoch: 1 step: 546, loss is 2.290388345718384\n",
      "epoch: 1 step: 547, loss is 2.3103017807006836\n",
      "epoch: 1 step: 548, loss is 2.2926976680755615\n",
      "epoch: 1 step: 549, loss is 2.314091920852661\n",
      "epoch: 1 step: 550, loss is 2.3236663341522217\n",
      "epoch: 1 step: 551, loss is 2.3111283779144287\n",
      "epoch: 1 step: 552, loss is 2.2878661155700684\n",
      "epoch: 1 step: 553, loss is 2.295464515686035\n",
      "epoch: 1 step: 554, loss is 2.316242218017578\n",
      "epoch: 1 step: 555, loss is 2.3051140308380127\n",
      "epoch: 1 step: 556, loss is 2.304276704788208\n",
      "epoch: 1 step: 557, loss is 2.314202308654785\n",
      "epoch: 1 step: 558, loss is 2.296567678451538\n",
      "epoch: 1 step: 559, loss is 2.2862741947174072\n",
      "epoch: 1 step: 560, loss is 2.3109798431396484\n",
      "epoch: 1 step: 561, loss is 2.3009626865386963\n",
      "epoch: 1 step: 562, loss is 2.3105862140655518\n",
      "epoch: 1 step: 563, loss is 2.2980153560638428\n",
      "epoch: 1 step: 564, loss is 2.2862448692321777\n",
      "epoch: 1 step: 565, loss is 2.3112056255340576\n",
      "epoch: 1 step: 566, loss is 2.3091418743133545\n",
      "epoch: 1 step: 567, loss is 2.288092613220215\n",
      "epoch: 1 step: 568, loss is 2.3105974197387695\n",
      "epoch: 1 step: 569, loss is 2.314033031463623\n",
      "epoch: 1 step: 570, loss is 2.304988384246826\n",
      "epoch: 1 step: 571, loss is 2.2956771850585938\n",
      "epoch: 1 step: 572, loss is 2.2786598205566406\n",
      "epoch: 1 step: 573, loss is 2.2905936241149902\n",
      "epoch: 1 step: 574, loss is 2.3100357055664062\n",
      "epoch: 1 step: 575, loss is 2.289289951324463\n",
      "epoch: 1 step: 576, loss is 2.29007625579834\n",
      "epoch: 1 step: 577, loss is 2.302442789077759\n",
      "epoch: 1 step: 578, loss is 2.280285358428955\n",
      "epoch: 1 step: 579, loss is 2.317324161529541\n",
      "epoch: 1 step: 580, loss is 2.307483673095703\n",
      "epoch: 1 step: 581, loss is 2.3110578060150146\n",
      "epoch: 1 step: 582, loss is 2.3009462356567383\n",
      "epoch: 1 step: 583, loss is 2.3112151622772217\n",
      "epoch: 1 step: 584, loss is 2.2824208736419678\n",
      "epoch: 1 step: 585, loss is 2.2995765209198\n",
      "epoch: 1 step: 586, loss is 2.319169759750366\n",
      "epoch: 1 step: 587, loss is 2.3036725521087646\n",
      "epoch: 1 step: 588, loss is 2.2877800464630127\n",
      "epoch: 1 step: 589, loss is 2.3055031299591064\n",
      "epoch: 1 step: 590, loss is 2.2903225421905518\n",
      "epoch: 1 step: 591, loss is 2.2899010181427\n",
      "epoch: 1 step: 592, loss is 2.3031227588653564\n",
      "epoch: 1 step: 593, loss is 2.29683256149292\n",
      "epoch: 1 step: 594, loss is 2.3034403324127197\n",
      "epoch: 1 step: 595, loss is 2.2926230430603027\n",
      "epoch: 1 step: 596, loss is 2.3108701705932617\n",
      "epoch: 1 step: 597, loss is 2.3025801181793213\n",
      "epoch: 1 step: 598, loss is 2.28942608833313\n",
      "epoch: 1 step: 599, loss is 2.265141725540161\n",
      "epoch: 1 step: 600, loss is 2.3127150535583496\n",
      "epoch: 1 step: 601, loss is 2.3057057857513428\n",
      "epoch: 1 step: 602, loss is 2.298579216003418\n",
      "epoch: 1 step: 603, loss is 2.283780336380005\n",
      "epoch: 1 step: 604, loss is 2.2996225357055664\n",
      "epoch: 1 step: 605, loss is 2.292207956314087\n",
      "epoch: 1 step: 606, loss is 2.2985422611236572\n",
      "epoch: 1 step: 607, loss is 2.3044533729553223\n",
      "epoch: 1 step: 608, loss is 2.310187339782715\n",
      "epoch: 1 step: 609, loss is 2.303194046020508\n",
      "epoch: 1 step: 610, loss is 2.3082923889160156\n",
      "epoch: 1 step: 611, loss is 2.285141706466675\n",
      "epoch: 1 step: 612, loss is 2.2908778190612793\n",
      "epoch: 1 step: 613, loss is 2.287292957305908\n",
      "epoch: 1 step: 614, loss is 2.304931402206421\n",
      "epoch: 1 step: 615, loss is 2.283600091934204\n",
      "epoch: 1 step: 616, loss is 2.2830259799957275\n",
      "epoch: 1 step: 617, loss is 2.3169941902160645\n",
      "epoch: 1 step: 618, loss is 2.3093807697296143\n",
      "epoch: 1 step: 619, loss is 2.2862842082977295\n",
      "epoch: 1 step: 620, loss is 2.315753698348999\n",
      "epoch: 1 step: 621, loss is 2.2949726581573486\n",
      "epoch: 1 step: 622, loss is 2.290860176086426\n",
      "epoch: 1 step: 623, loss is 2.2925732135772705\n",
      "epoch: 1 step: 624, loss is 2.293975591659546\n",
      "epoch: 1 step: 625, loss is 2.2931129932403564\n",
      "epoch: 1 step: 626, loss is 2.303317070007324\n",
      "epoch: 1 step: 627, loss is 2.2981958389282227\n",
      "epoch: 1 step: 628, loss is 2.289992332458496\n",
      "epoch: 1 step: 629, loss is 2.295720338821411\n",
      "epoch: 1 step: 630, loss is 2.292863130569458\n",
      "epoch: 1 step: 631, loss is 2.2860724925994873\n",
      "epoch: 1 step: 632, loss is 2.2868149280548096\n",
      "epoch: 1 step: 633, loss is 2.2765889167785645\n",
      "epoch: 1 step: 634, loss is 2.2896876335144043\n",
      "epoch: 1 step: 635, loss is 2.3012702465057373\n",
      "epoch: 1 step: 636, loss is 2.292835235595703\n",
      "epoch: 1 step: 637, loss is 2.3060455322265625\n",
      "epoch: 1 step: 638, loss is 2.295090913772583\n",
      "epoch: 1 step: 639, loss is 2.2899580001831055\n",
      "epoch: 1 step: 640, loss is 2.3004093170166016\n",
      "epoch: 1 step: 641, loss is 2.2840187549591064\n",
      "epoch: 1 step: 642, loss is 2.285508394241333\n",
      "epoch: 1 step: 643, loss is 2.2734673023223877\n",
      "epoch: 1 step: 644, loss is 2.3044326305389404\n",
      "epoch: 1 step: 645, loss is 2.28385853767395\n",
      "epoch: 1 step: 646, loss is 2.2798068523406982\n",
      "epoch: 1 step: 647, loss is 2.292475938796997\n",
      "epoch: 1 step: 648, loss is 2.2827720642089844\n",
      "epoch: 1 step: 649, loss is 2.270216464996338\n",
      "epoch: 1 step: 650, loss is 2.277484655380249\n",
      "epoch: 1 step: 651, loss is 2.271314859390259\n",
      "epoch: 1 step: 652, loss is 2.259005546569824\n",
      "epoch: 1 step: 653, loss is 2.2830352783203125\n",
      "epoch: 1 step: 654, loss is 2.2808332443237305\n",
      "epoch: 1 step: 655, loss is 2.2867095470428467\n",
      "epoch: 1 step: 656, loss is 2.2623679637908936\n",
      "epoch: 1 step: 657, loss is 2.284043550491333\n",
      "epoch: 1 step: 658, loss is 2.269631862640381\n",
      "epoch: 1 step: 659, loss is 2.2645621299743652\n",
      "epoch: 1 step: 660, loss is 2.2633216381073\n",
      "epoch: 1 step: 661, loss is 2.2391867637634277\n",
      "epoch: 1 step: 662, loss is 2.2580833435058594\n",
      "epoch: 1 step: 663, loss is 2.25986385345459\n",
      "epoch: 1 step: 664, loss is 2.2742013931274414\n",
      "epoch: 1 step: 665, loss is 2.2464001178741455\n",
      "epoch: 1 step: 666, loss is 2.254558801651001\n",
      "epoch: 1 step: 667, loss is 2.2169651985168457\n",
      "epoch: 1 step: 668, loss is 2.2683613300323486\n",
      "epoch: 1 step: 669, loss is 2.2362265586853027\n",
      "epoch: 1 step: 670, loss is 2.1959497928619385\n",
      "epoch: 1 step: 671, loss is 2.2088358402252197\n",
      "epoch: 1 step: 672, loss is 2.218937397003174\n",
      "epoch: 1 step: 673, loss is 2.232344627380371\n",
      "epoch: 1 step: 674, loss is 2.2117297649383545\n",
      "epoch: 1 step: 675, loss is 2.1970601081848145\n",
      "epoch: 1 step: 676, loss is 2.208468437194824\n",
      "epoch: 1 step: 677, loss is 2.1928787231445312\n",
      "epoch: 1 step: 678, loss is 2.124633312225342\n",
      "epoch: 1 step: 679, loss is 2.209724187850952\n",
      "epoch: 1 step: 680, loss is 2.136308193206787\n",
      "epoch: 1 step: 681, loss is 2.174531936645508\n",
      "epoch: 1 step: 682, loss is 2.119032859802246\n",
      "epoch: 1 step: 683, loss is 2.138624668121338\n",
      "epoch: 1 step: 684, loss is 1.9909433126449585\n",
      "epoch: 1 step: 685, loss is 1.890756368637085\n",
      "epoch: 1 step: 686, loss is 2.0309245586395264\n",
      "epoch: 1 step: 687, loss is 1.9566426277160645\n",
      "epoch: 1 step: 688, loss is 2.058694362640381\n",
      "epoch: 1 step: 689, loss is 1.8172023296356201\n",
      "epoch: 1 step: 690, loss is 1.8890173435211182\n",
      "epoch: 1 step: 691, loss is 1.7112764120101929\n",
      "epoch: 1 step: 692, loss is 1.8335975408554077\n",
      "epoch: 1 step: 693, loss is 1.9214389324188232\n",
      "epoch: 1 step: 694, loss is 1.7852548360824585\n",
      "epoch: 1 step: 695, loss is 1.4656367301940918\n",
      "epoch: 1 step: 696, loss is 1.9182645082473755\n",
      "epoch: 1 step: 697, loss is 1.4087371826171875\n",
      "epoch: 1 step: 698, loss is 1.7444665431976318\n",
      "epoch: 1 step: 699, loss is 1.6664260625839233\n",
      "epoch: 1 step: 700, loss is 1.3730672597885132\n",
      "epoch: 1 step: 701, loss is 1.5341354608535767\n",
      "epoch: 1 step: 702, loss is 1.302021861076355\n",
      "epoch: 1 step: 703, loss is 1.619429588317871\n",
      "epoch: 1 step: 704, loss is 1.777575135231018\n",
      "epoch: 1 step: 705, loss is 0.9295664429664612\n",
      "epoch: 1 step: 706, loss is 1.4055606126785278\n",
      "epoch: 1 step: 707, loss is 0.8778862357139587\n",
      "epoch: 1 step: 708, loss is 1.2315998077392578\n",
      "epoch: 1 step: 709, loss is 1.26185142993927\n",
      "epoch: 1 step: 710, loss is 1.2441542148590088\n",
      "epoch: 1 step: 711, loss is 1.1786261796951294\n",
      "epoch: 1 step: 712, loss is 1.1317803859710693\n",
      "epoch: 1 step: 713, loss is 1.0791500806808472\n",
      "epoch: 1 step: 714, loss is 1.0319441556930542\n",
      "epoch: 1 step: 715, loss is 1.150903582572937\n",
      "epoch: 1 step: 716, loss is 1.1201386451721191\n",
      "epoch: 1 step: 717, loss is 1.0007240772247314\n",
      "epoch: 1 step: 718, loss is 1.099320411682129\n",
      "epoch: 1 step: 719, loss is 0.8801460266113281\n",
      "epoch: 1 step: 720, loss is 0.8738446235656738\n",
      "epoch: 1 step: 721, loss is 0.9356545209884644\n",
      "epoch: 1 step: 722, loss is 1.034139633178711\n",
      "epoch: 1 step: 723, loss is 0.772243082523346\n",
      "epoch: 1 step: 724, loss is 1.1328052282333374\n",
      "epoch: 1 step: 725, loss is 0.735758900642395\n",
      "epoch: 1 step: 726, loss is 1.1691569089889526\n",
      "epoch: 1 step: 727, loss is 1.0045063495635986\n",
      "epoch: 1 step: 728, loss is 2.1371994018554688\n",
      "epoch: 1 step: 729, loss is 0.6552721858024597\n",
      "epoch: 1 step: 730, loss is 1.020827054977417\n",
      "epoch: 1 step: 731, loss is 1.8630133867263794\n",
      "epoch: 1 step: 732, loss is 1.188148856163025\n",
      "epoch: 1 step: 733, loss is 0.7981494665145874\n",
      "epoch: 1 step: 734, loss is 0.9005683064460754\n",
      "epoch: 1 step: 735, loss is 1.0499366521835327\n",
      "epoch: 1 step: 736, loss is 0.9582988619804382\n",
      "epoch: 1 step: 737, loss is 1.02993643283844\n",
      "epoch: 1 step: 738, loss is 0.7551367878913879\n",
      "epoch: 1 step: 739, loss is 1.0104522705078125\n",
      "epoch: 1 step: 740, loss is 1.1116862297058105\n",
      "epoch: 1 step: 741, loss is 0.8604364991188049\n",
      "epoch: 1 step: 742, loss is 1.2613637447357178\n",
      "epoch: 1 step: 743, loss is 0.9318724274635315\n",
      "epoch: 1 step: 744, loss is 0.6670106053352356\n",
      "epoch: 1 step: 745, loss is 1.094822645187378\n",
      "epoch: 1 step: 746, loss is 0.8427541851997375\n",
      "epoch: 1 step: 747, loss is 0.8230244517326355\n",
      "epoch: 1 step: 748, loss is 0.8268024325370789\n",
      "epoch: 1 step: 749, loss is 0.8298549652099609\n",
      "epoch: 1 step: 750, loss is 0.804953396320343\n",
      "epoch: 1 step: 751, loss is 1.2679606676101685\n",
      "epoch: 1 step: 752, loss is 0.43772274255752563\n",
      "epoch: 1 step: 753, loss is 0.6003434062004089\n",
      "epoch: 1 step: 754, loss is 1.1281181573867798\n",
      "epoch: 1 step: 755, loss is 1.345073938369751\n",
      "epoch: 1 step: 756, loss is 0.6184720993041992\n",
      "epoch: 1 step: 757, loss is 0.6112804412841797\n",
      "epoch: 1 step: 758, loss is 0.7586216330528259\n",
      "epoch: 1 step: 759, loss is 1.0086050033569336\n",
      "epoch: 1 step: 760, loss is 0.9505253434181213\n",
      "epoch: 1 step: 761, loss is 0.7727802991867065\n",
      "epoch: 1 step: 762, loss is 0.5200885534286499\n",
      "epoch: 1 step: 763, loss is 0.5987709164619446\n",
      "epoch: 1 step: 764, loss is 0.5244489312171936\n",
      "epoch: 1 step: 765, loss is 0.7105008363723755\n",
      "epoch: 1 step: 766, loss is 0.7234772443771362\n",
      "epoch: 1 step: 767, loss is 0.7017995119094849\n",
      "epoch: 1 step: 768, loss is 0.5026004314422607\n",
      "epoch: 1 step: 769, loss is 0.1957719773054123\n",
      "epoch: 1 step: 770, loss is 0.8776431679725647\n",
      "epoch: 1 step: 771, loss is 0.5645290017127991\n",
      "epoch: 1 step: 772, loss is 0.8606911301612854\n",
      "epoch: 1 step: 773, loss is 1.1597504615783691\n",
      "epoch: 1 step: 774, loss is 0.64677494764328\n",
      "epoch: 1 step: 775, loss is 0.6492580771446228\n",
      "epoch: 1 step: 776, loss is 0.8630515336990356\n",
      "epoch: 1 step: 777, loss is 0.4491908550262451\n",
      "epoch: 1 step: 778, loss is 0.3902565836906433\n",
      "epoch: 1 step: 779, loss is 0.42643213272094727\n",
      "epoch: 1 step: 780, loss is 0.7547020316123962\n",
      "epoch: 1 step: 781, loss is 0.6222212910652161\n",
      "epoch: 1 step: 782, loss is 0.5721015334129333\n",
      "epoch: 1 step: 783, loss is 0.4350157678127289\n",
      "epoch: 1 step: 784, loss is 0.45077165961265564\n",
      "epoch: 1 step: 785, loss is 0.6476967334747314\n",
      "epoch: 1 step: 786, loss is 0.3560514450073242\n",
      "epoch: 1 step: 787, loss is 0.43182381987571716\n",
      "epoch: 1 step: 788, loss is 0.6546040177345276\n",
      "epoch: 1 step: 789, loss is 0.8177648186683655\n",
      "epoch: 1 step: 790, loss is 0.3340310752391815\n",
      "epoch: 1 step: 791, loss is 0.508831262588501\n",
      "epoch: 1 step: 792, loss is 0.5351355671882629\n",
      "epoch: 1 step: 793, loss is 0.3599211275577545\n",
      "epoch: 1 step: 794, loss is 0.7838361263275146\n",
      "epoch: 1 step: 795, loss is 0.47467708587646484\n",
      "epoch: 1 step: 796, loss is 0.5223580002784729\n",
      "epoch: 1 step: 797, loss is 0.7015331387519836\n",
      "epoch: 1 step: 798, loss is 0.6564002633094788\n",
      "epoch: 1 step: 799, loss is 0.5091893076896667\n",
      "epoch: 1 step: 800, loss is 0.5495553612709045\n",
      "epoch: 1 step: 801, loss is 0.40269508957862854\n",
      "epoch: 1 step: 802, loss is 0.5580917000770569\n",
      "epoch: 1 step: 803, loss is 0.4527316689491272\n",
      "epoch: 1 step: 804, loss is 0.30679410696029663\n",
      "epoch: 1 step: 805, loss is 0.3115212321281433\n",
      "epoch: 1 step: 806, loss is 0.6939523816108704\n",
      "epoch: 1 step: 807, loss is 0.3089198172092438\n",
      "epoch: 1 step: 808, loss is 0.35263893008232117\n",
      "epoch: 1 step: 809, loss is 0.5553523302078247\n",
      "epoch: 1 step: 810, loss is 0.4341467618942261\n",
      "epoch: 1 step: 811, loss is 0.5179309248924255\n",
      "epoch: 1 step: 812, loss is 0.4363201856613159\n",
      "epoch: 1 step: 813, loss is 0.4096866846084595\n",
      "epoch: 1 step: 814, loss is 0.5451347827911377\n",
      "epoch: 1 step: 815, loss is 0.43318137526512146\n",
      "epoch: 1 step: 816, loss is 0.3263327181339264\n",
      "epoch: 1 step: 817, loss is 0.13919386267662048\n",
      "epoch: 1 step: 818, loss is 0.5784304141998291\n",
      "epoch: 1 step: 819, loss is 0.6660480499267578\n",
      "epoch: 1 step: 820, loss is 0.4080982208251953\n",
      "epoch: 1 step: 821, loss is 0.5224851965904236\n",
      "epoch: 1 step: 822, loss is 0.4844229519367218\n",
      "epoch: 1 step: 823, loss is 0.4444928467273712\n",
      "epoch: 1 step: 824, loss is 0.2952994108200073\n",
      "epoch: 1 step: 825, loss is 0.3409111499786377\n",
      "epoch: 1 step: 826, loss is 0.45473894476890564\n",
      "epoch: 1 step: 827, loss is 0.7947388887405396\n",
      "epoch: 1 step: 828, loss is 0.40086281299591064\n",
      "epoch: 1 step: 829, loss is 0.8177594542503357\n",
      "epoch: 1 step: 830, loss is 0.4537143409252167\n",
      "epoch: 1 step: 831, loss is 0.3145306706428528\n",
      "epoch: 1 step: 832, loss is 0.39350229501724243\n",
      "epoch: 1 step: 833, loss is 0.21884949505329132\n",
      "epoch: 1 step: 834, loss is 0.20617607235908508\n",
      "epoch: 1 step: 835, loss is 0.7454842925071716\n",
      "epoch: 1 step: 836, loss is 0.3025256395339966\n",
      "epoch: 1 step: 837, loss is 0.47250789403915405\n",
      "epoch: 1 step: 838, loss is 0.19348378479480743\n",
      "epoch: 1 step: 839, loss is 0.13543230295181274\n",
      "epoch: 1 step: 840, loss is 0.3486286997795105\n",
      "epoch: 1 step: 841, loss is 0.7177671194076538\n",
      "epoch: 1 step: 842, loss is 0.21737328171730042\n",
      "epoch: 1 step: 843, loss is 0.38396283984184265\n",
      "epoch: 1 step: 844, loss is 0.6436299681663513\n",
      "epoch: 1 step: 845, loss is 0.2756488621234894\n",
      "epoch: 1 step: 846, loss is 0.49621307849884033\n",
      "epoch: 1 step: 847, loss is 0.2607693374156952\n",
      "epoch: 1 step: 848, loss is 0.1628013402223587\n",
      "epoch: 1 step: 849, loss is 0.6410407423973083\n",
      "epoch: 1 step: 850, loss is 0.48901858925819397\n",
      "epoch: 1 step: 851, loss is 0.21191713213920593\n",
      "epoch: 1 step: 852, loss is 0.3260028660297394\n",
      "epoch: 1 step: 853, loss is 0.35375893115997314\n",
      "epoch: 1 step: 854, loss is 0.32032832503318787\n",
      "epoch: 1 step: 855, loss is 0.2599872052669525\n",
      "epoch: 1 step: 856, loss is 0.7131205201148987\n",
      "epoch: 1 step: 857, loss is 0.4252757728099823\n",
      "epoch: 1 step: 858, loss is 0.28062641620635986\n",
      "epoch: 1 step: 859, loss is 0.31899306178092957\n",
      "epoch: 1 step: 860, loss is 0.30814769864082336\n",
      "epoch: 1 step: 861, loss is 0.38754934072494507\n",
      "epoch: 1 step: 862, loss is 0.46701064705848694\n",
      "epoch: 1 step: 863, loss is 0.33883199095726013\n",
      "epoch: 1 step: 864, loss is 0.3407054841518402\n",
      "epoch: 1 step: 865, loss is 0.17993322014808655\n",
      "epoch: 1 step: 866, loss is 0.2655177712440491\n",
      "epoch: 1 step: 867, loss is 0.1029190868139267\n",
      "epoch: 1 step: 868, loss is 0.38445404171943665\n",
      "epoch: 1 step: 869, loss is 0.22606894373893738\n",
      "epoch: 1 step: 870, loss is 0.3021239936351776\n",
      "epoch: 1 step: 871, loss is 0.30929213762283325\n",
      "epoch: 1 step: 872, loss is 0.4094090461730957\n",
      "epoch: 1 step: 873, loss is 0.48348698019981384\n",
      "epoch: 1 step: 874, loss is 0.16281640529632568\n",
      "epoch: 1 step: 875, loss is 0.1763940155506134\n",
      "epoch: 1 step: 876, loss is 0.31029534339904785\n",
      "epoch: 1 step: 877, loss is 0.13246242702007294\n",
      "epoch: 1 step: 878, loss is 0.3921057879924774\n",
      "epoch: 1 step: 879, loss is 0.2831862270832062\n",
      "epoch: 1 step: 880, loss is 0.634063184261322\n",
      "epoch: 1 step: 881, loss is 0.3774031698703766\n",
      "epoch: 1 step: 882, loss is 0.09828922897577286\n",
      "epoch: 1 step: 883, loss is 0.40207165479660034\n",
      "epoch: 1 step: 884, loss is 0.5023130774497986\n",
      "epoch: 1 step: 885, loss is 0.31549879908561707\n",
      "epoch: 1 step: 886, loss is 0.2248929738998413\n",
      "epoch: 1 step: 887, loss is 0.42510178685188293\n",
      "epoch: 1 step: 888, loss is 0.3488833010196686\n",
      "epoch: 1 step: 889, loss is 0.28209400177001953\n",
      "epoch: 1 step: 890, loss is 0.44467127323150635\n",
      "epoch: 1 step: 891, loss is 0.12899087369441986\n",
      "epoch: 1 step: 892, loss is 0.34543558955192566\n",
      "epoch: 1 step: 893, loss is 0.4673113524913788\n",
      "epoch: 1 step: 894, loss is 0.3633474111557007\n",
      "epoch: 1 step: 895, loss is 0.29018229246139526\n",
      "epoch: 1 step: 896, loss is 0.14119429886341095\n",
      "epoch: 1 step: 897, loss is 0.3979980945587158\n",
      "epoch: 1 step: 898, loss is 0.3096022307872772\n",
      "epoch: 1 step: 899, loss is 0.4768921434879303\n",
      "epoch: 1 step: 900, loss is 0.4978300631046295\n",
      "epoch: 1 step: 901, loss is 0.5817214250564575\n",
      "epoch: 1 step: 902, loss is 0.6568655967712402\n",
      "epoch: 1 step: 903, loss is 0.5475437641143799\n",
      "epoch: 1 step: 904, loss is 0.19248445332050323\n",
      "epoch: 1 step: 905, loss is 0.4466560482978821\n",
      "epoch: 1 step: 906, loss is 0.5952576994895935\n",
      "epoch: 1 step: 907, loss is 0.4894142150878906\n",
      "epoch: 1 step: 908, loss is 0.24737749993801117\n",
      "epoch: 1 step: 909, loss is 0.3566032648086548\n",
      "epoch: 1 step: 910, loss is 0.5737332105636597\n",
      "epoch: 1 step: 911, loss is 0.4533928632736206\n",
      "epoch: 1 step: 912, loss is 0.35646194219589233\n",
      "epoch: 1 step: 913, loss is 0.3002392053604126\n",
      "epoch: 1 step: 914, loss is 0.09720907360315323\n",
      "epoch: 1 step: 915, loss is 0.8901114463806152\n",
      "epoch: 1 step: 916, loss is 0.15447306632995605\n",
      "epoch: 1 step: 917, loss is 0.24160359799861908\n",
      "epoch: 1 step: 918, loss is 0.46681150794029236\n",
      "epoch: 1 step: 919, loss is 0.39100509881973267\n",
      "epoch: 1 step: 920, loss is 0.22708433866500854\n",
      "epoch: 1 step: 921, loss is 0.3224775791168213\n",
      "epoch: 1 step: 922, loss is 0.29997870326042175\n",
      "epoch: 1 step: 923, loss is 0.25492337346076965\n",
      "epoch: 1 step: 924, loss is 0.3398550748825073\n",
      "epoch: 1 step: 925, loss is 0.19433724880218506\n",
      "epoch: 1 step: 926, loss is 0.496977299451828\n",
      "epoch: 1 step: 927, loss is 0.1879960149526596\n",
      "epoch: 1 step: 928, loss is 0.19865256547927856\n",
      "epoch: 1 step: 929, loss is 0.25477591156959534\n",
      "epoch: 1 step: 930, loss is 0.22131691873073578\n",
      "epoch: 1 step: 931, loss is 0.18127425014972687\n",
      "epoch: 1 step: 932, loss is 0.23851387202739716\n",
      "epoch: 1 step: 933, loss is 0.6174602508544922\n",
      "epoch: 1 step: 934, loss is 0.2202915996313095\n",
      "epoch: 1 step: 935, loss is 0.2935353219509125\n",
      "epoch: 1 step: 936, loss is 0.3657569885253906\n",
      "epoch: 1 step: 937, loss is 0.22745580971240997\n",
      "epoch: 1 step: 938, loss is 0.1369820088148117\n",
      "epoch: 1 step: 939, loss is 0.38719606399536133\n",
      "epoch: 1 step: 940, loss is 0.2283996343612671\n",
      "epoch: 1 step: 941, loss is 0.35116273164749146\n",
      "epoch: 1 step: 942, loss is 0.199864462018013\n",
      "epoch: 1 step: 943, loss is 0.1549239307641983\n",
      "epoch: 1 step: 944, loss is 0.23284751176834106\n",
      "epoch: 1 step: 945, loss is 0.25251585245132446\n",
      "epoch: 1 step: 946, loss is 0.31854069232940674\n",
      "epoch: 1 step: 947, loss is 0.23286131024360657\n",
      "epoch: 1 step: 948, loss is 0.1826384961605072\n",
      "epoch: 1 step: 949, loss is 0.06623370945453644\n",
      "epoch: 1 step: 950, loss is 0.10797014832496643\n",
      "epoch: 1 step: 951, loss is 0.18387861549854279\n",
      "epoch: 1 step: 952, loss is 0.09271986782550812\n",
      "epoch: 1 step: 953, loss is 0.13708999752998352\n",
      "epoch: 1 step: 954, loss is 0.2048870027065277\n",
      "epoch: 1 step: 955, loss is 0.06186994910240173\n",
      "epoch: 1 step: 956, loss is 0.28422728180885315\n",
      "epoch: 1 step: 957, loss is 0.3862917423248291\n",
      "epoch: 1 step: 958, loss is 0.20145846903324127\n",
      "epoch: 1 step: 959, loss is 0.45267581939697266\n",
      "epoch: 1 step: 960, loss is 0.4274875521659851\n",
      "epoch: 1 step: 961, loss is 0.24067893624305725\n",
      "epoch: 1 step: 962, loss is 0.11540912836790085\n",
      "epoch: 1 step: 963, loss is 0.5715432167053223\n",
      "epoch: 1 step: 964, loss is 0.2016221135854721\n",
      "epoch: 1 step: 965, loss is 0.201619952917099\n",
      "epoch: 1 step: 966, loss is 0.277902752161026\n",
      "epoch: 1 step: 967, loss is 0.5844433903694153\n",
      "epoch: 1 step: 968, loss is 0.33193883299827576\n",
      "epoch: 1 step: 969, loss is 0.3898077607154846\n",
      "epoch: 1 step: 970, loss is 0.2584804892539978\n",
      "epoch: 1 step: 971, loss is 0.17984096705913544\n",
      "epoch: 1 step: 972, loss is 0.16969828307628632\n",
      "epoch: 1 step: 973, loss is 0.38038209080696106\n",
      "epoch: 1 step: 974, loss is 0.1389043927192688\n",
      "epoch: 1 step: 975, loss is 0.27394628524780273\n",
      "epoch: 1 step: 976, loss is 0.1890878677368164\n",
      "epoch: 1 step: 977, loss is 0.32495200634002686\n",
      "epoch: 1 step: 978, loss is 0.060732290148735046\n",
      "epoch: 1 step: 979, loss is 0.09394335746765137\n",
      "epoch: 1 step: 980, loss is 0.3103177845478058\n",
      "epoch: 1 step: 981, loss is 0.2910913825035095\n",
      "epoch: 1 step: 982, loss is 0.2215500771999359\n",
      "epoch: 1 step: 983, loss is 0.15241706371307373\n",
      "epoch: 1 step: 984, loss is 0.45020559430122375\n",
      "epoch: 1 step: 985, loss is 0.09876538813114166\n",
      "epoch: 1 step: 986, loss is 0.15702727437019348\n",
      "epoch: 1 step: 987, loss is 0.19448921084403992\n",
      "epoch: 1 step: 988, loss is 0.12424527108669281\n",
      "epoch: 1 step: 989, loss is 0.2564897835254669\n",
      "epoch: 1 step: 990, loss is 0.0726756677031517\n",
      "epoch: 1 step: 991, loss is 0.06765864789485931\n",
      "epoch: 1 step: 992, loss is 0.10348297655582428\n",
      "epoch: 1 step: 993, loss is 0.2705414295196533\n",
      "epoch: 1 step: 994, loss is 0.16437548398971558\n",
      "epoch: 1 step: 995, loss is 0.21528641879558563\n",
      "epoch: 1 step: 996, loss is 0.5037657618522644\n",
      "epoch: 1 step: 997, loss is 0.04623887687921524\n",
      "epoch: 1 step: 998, loss is 0.30963489413261414\n",
      "epoch: 1 step: 999, loss is 0.20151272416114807\n",
      "epoch: 1 step: 1000, loss is 0.20185914635658264\n",
      "epoch: 1 step: 1001, loss is 0.14244738221168518\n",
      "epoch: 1 step: 1002, loss is 0.1110958606004715\n",
      "epoch: 1 step: 1003, loss is 0.09029006958007812\n",
      "epoch: 1 step: 1004, loss is 0.23748280107975006\n",
      "epoch: 1 step: 1005, loss is 0.07472605258226395\n",
      "epoch: 1 step: 1006, loss is 0.26970160007476807\n",
      "epoch: 1 step: 1007, loss is 0.40340274572372437\n",
      "epoch: 1 step: 1008, loss is 0.14273308217525482\n",
      "epoch: 1 step: 1009, loss is 0.2208847552537918\n",
      "epoch: 1 step: 1010, loss is 0.08727571368217468\n",
      "epoch: 1 step: 1011, loss is 0.10852532833814621\n",
      "epoch: 1 step: 1012, loss is 0.35360294580459595\n",
      "epoch: 1 step: 1013, loss is 0.15314899384975433\n",
      "epoch: 1 step: 1014, loss is 0.18668866157531738\n",
      "epoch: 1 step: 1015, loss is 0.7936435341835022\n",
      "epoch: 1 step: 1016, loss is 0.13722865283489227\n",
      "epoch: 1 step: 1017, loss is 0.10177866369485855\n",
      "epoch: 1 step: 1018, loss is 0.03878842666745186\n",
      "epoch: 1 step: 1019, loss is 0.19924253225326538\n",
      "epoch: 1 step: 1020, loss is 0.21959756314754486\n",
      "epoch: 1 step: 1021, loss is 0.4989348351955414\n",
      "epoch: 1 step: 1022, loss is 0.18428775668144226\n",
      "epoch: 1 step: 1023, loss is 0.14031417667865753\n",
      "epoch: 1 step: 1024, loss is 0.6128124594688416\n",
      "epoch: 1 step: 1025, loss is 0.30204659700393677\n",
      "epoch: 1 step: 1026, loss is 0.26506245136260986\n",
      "epoch: 1 step: 1027, loss is 0.2515213191509247\n",
      "epoch: 1 step: 1028, loss is 0.14638876914978027\n",
      "epoch: 1 step: 1029, loss is 0.016753263771533966\n",
      "epoch: 1 step: 1030, loss is 0.09276705235242844\n",
      "epoch: 1 step: 1031, loss is 0.19922465085983276\n",
      "epoch: 1 step: 1032, loss is 0.21616233885288239\n",
      "epoch: 1 step: 1033, loss is 0.12950670719146729\n",
      "epoch: 1 step: 1034, loss is 0.13280881941318512\n",
      "epoch: 1 step: 1035, loss is 0.14084288477897644\n",
      "epoch: 1 step: 1036, loss is 0.08516968041658401\n",
      "epoch: 1 step: 1037, loss is 0.317322701215744\n",
      "epoch: 1 step: 1038, loss is 0.13782066106796265\n",
      "epoch: 1 step: 1039, loss is 0.06395179778337479\n",
      "epoch: 1 step: 1040, loss is 0.326069712638855\n",
      "epoch: 1 step: 1041, loss is 0.3123348355293274\n",
      "epoch: 1 step: 1042, loss is 0.2948625385761261\n",
      "epoch: 1 step: 1043, loss is 0.10332103073596954\n",
      "epoch: 1 step: 1044, loss is 0.1881849616765976\n",
      "epoch: 1 step: 1045, loss is 0.1803724467754364\n",
      "epoch: 1 step: 1046, loss is 0.09297910332679749\n",
      "epoch: 1 step: 1047, loss is 0.03460188955068588\n",
      "epoch: 1 step: 1048, loss is 0.1619567722082138\n",
      "epoch: 1 step: 1049, loss is 0.3113754689693451\n",
      "epoch: 1 step: 1050, loss is 0.20782072842121124\n",
      "epoch: 1 step: 1051, loss is 0.3102978765964508\n",
      "epoch: 1 step: 1052, loss is 0.2050488144159317\n",
      "epoch: 1 step: 1053, loss is 0.07726415991783142\n",
      "epoch: 1 step: 1054, loss is 0.341150164604187\n",
      "epoch: 1 step: 1055, loss is 0.051239874213933945\n",
      "epoch: 1 step: 1056, loss is 0.0466717928647995\n",
      "epoch: 1 step: 1057, loss is 0.02641153894364834\n",
      "epoch: 1 step: 1058, loss is 0.13332036137580872\n",
      "epoch: 1 step: 1059, loss is 0.6354063153266907\n",
      "epoch: 1 step: 1060, loss is 0.31980443000793457\n",
      "epoch: 1 step: 1061, loss is 0.26401248574256897\n",
      "epoch: 1 step: 1062, loss is 0.4100039601325989\n",
      "epoch: 1 step: 1063, loss is 0.36568838357925415\n",
      "epoch: 1 step: 1064, loss is 0.1247418001294136\n",
      "epoch: 1 step: 1065, loss is 0.0446767695248127\n",
      "epoch: 1 step: 1066, loss is 0.20338407158851624\n",
      "epoch: 1 step: 1067, loss is 0.22155790030956268\n",
      "epoch: 1 step: 1068, loss is 0.33727186918258667\n",
      "epoch: 1 step: 1069, loss is 0.28034985065460205\n",
      "epoch: 1 step: 1070, loss is 0.17166371643543243\n",
      "epoch: 1 step: 1071, loss is 0.31504151225090027\n",
      "epoch: 1 step: 1072, loss is 0.20489391684532166\n",
      "epoch: 1 step: 1073, loss is 0.28352099657058716\n",
      "epoch: 1 step: 1074, loss is 0.5540204048156738\n",
      "epoch: 1 step: 1075, loss is 0.45525771379470825\n",
      "epoch: 1 step: 1076, loss is 0.2604258060455322\n",
      "epoch: 1 step: 1077, loss is 0.2566576302051544\n",
      "epoch: 1 step: 1078, loss is 0.24259625375270844\n",
      "epoch: 1 step: 1079, loss is 0.1805592179298401\n",
      "epoch: 1 step: 1080, loss is 0.154976949095726\n",
      "epoch: 1 step: 1081, loss is 0.4229210615158081\n",
      "epoch: 1 step: 1082, loss is 0.1959545612335205\n",
      "epoch: 1 step: 1083, loss is 0.2041374146938324\n",
      "epoch: 1 step: 1084, loss is 0.05025987699627876\n",
      "epoch: 1 step: 1085, loss is 0.29802897572517395\n",
      "epoch: 1 step: 1086, loss is 0.23416805267333984\n",
      "epoch: 1 step: 1087, loss is 0.2206043154001236\n",
      "epoch: 1 step: 1088, loss is 0.19449113309383392\n",
      "epoch: 1 step: 1089, loss is 0.3827211856842041\n",
      "epoch: 1 step: 1090, loss is 0.20310118794441223\n",
      "epoch: 1 step: 1091, loss is 0.17948000133037567\n",
      "epoch: 1 step: 1092, loss is 0.039455972611904144\n",
      "epoch: 1 step: 1093, loss is 0.04627881199121475\n",
      "epoch: 1 step: 1094, loss is 0.10044437646865845\n",
      "epoch: 1 step: 1095, loss is 0.38227149844169617\n",
      "epoch: 1 step: 1096, loss is 0.10771895200014114\n",
      "epoch: 1 step: 1097, loss is 0.10449893027544022\n",
      "epoch: 1 step: 1098, loss is 0.3640819191932678\n",
      "epoch: 1 step: 1099, loss is 0.09880021214485168\n",
      "epoch: 1 step: 1100, loss is 0.13910871744155884\n",
      "epoch: 1 step: 1101, loss is 0.05034694820642471\n",
      "epoch: 1 step: 1102, loss is 0.041101887822151184\n",
      "epoch: 1 step: 1103, loss is 0.07898622751235962\n",
      "epoch: 1 step: 1104, loss is 0.0856475979089737\n",
      "epoch: 1 step: 1105, loss is 0.10756667703390121\n",
      "epoch: 1 step: 1106, loss is 0.15597698092460632\n",
      "epoch: 1 step: 1107, loss is 0.14775820076465607\n",
      "epoch: 1 step: 1108, loss is 0.15596038103103638\n",
      "epoch: 1 step: 1109, loss is 0.09898112714290619\n",
      "epoch: 1 step: 1110, loss is 0.01577753573656082\n",
      "epoch: 1 step: 1111, loss is 0.0795055627822876\n",
      "epoch: 1 step: 1112, loss is 0.29182499647140503\n",
      "epoch: 1 step: 1113, loss is 0.27805888652801514\n",
      "epoch: 1 step: 1114, loss is 0.04192778468132019\n",
      "epoch: 1 step: 1115, loss is 0.18612037599086761\n",
      "epoch: 1 step: 1116, loss is 0.5392031669616699\n",
      "epoch: 1 step: 1117, loss is 0.13694104552268982\n",
      "epoch: 1 step: 1118, loss is 0.24587811529636383\n",
      "epoch: 1 step: 1119, loss is 0.0719541385769844\n",
      "epoch: 1 step: 1120, loss is 0.335864394903183\n",
      "epoch: 1 step: 1121, loss is 0.1487569510936737\n",
      "epoch: 1 step: 1122, loss is 0.20661470293998718\n",
      "epoch: 1 step: 1123, loss is 0.058064430952072144\n",
      "epoch: 1 step: 1124, loss is 0.008054699748754501\n",
      "epoch: 1 step: 1125, loss is 0.19106197357177734\n",
      "epoch: 1 step: 1126, loss is 0.0804765522480011\n",
      "epoch: 1 step: 1127, loss is 0.2864229679107666\n",
      "epoch: 1 step: 1128, loss is 0.1316288858652115\n",
      "epoch: 1 step: 1129, loss is 0.46503445506095886\n",
      "epoch: 1 step: 1130, loss is 0.1070573702454567\n",
      "epoch: 1 step: 1131, loss is 0.11997302621603012\n",
      "epoch: 1 step: 1132, loss is 0.38616570830345154\n",
      "epoch: 1 step: 1133, loss is 0.056993212550878525\n",
      "epoch: 1 step: 1134, loss is 0.16811221837997437\n",
      "epoch: 1 step: 1135, loss is 0.21137402951717377\n",
      "epoch: 1 step: 1136, loss is 0.2371930032968521\n",
      "epoch: 1 step: 1137, loss is 0.14638127386569977\n",
      "epoch: 1 step: 1138, loss is 0.24651256203651428\n",
      "epoch: 1 step: 1139, loss is 0.06259404867887497\n",
      "epoch: 1 step: 1140, loss is 0.2033471167087555\n",
      "epoch: 1 step: 1141, loss is 0.025144794955849648\n",
      "epoch: 1 step: 1142, loss is 0.23729117214679718\n",
      "epoch: 1 step: 1143, loss is 0.16374321281909943\n",
      "epoch: 1 step: 1144, loss is 0.007796393241733313\n",
      "epoch: 1 step: 1145, loss is 0.06894183158874512\n",
      "epoch: 1 step: 1146, loss is 0.17448867857456207\n",
      "epoch: 1 step: 1147, loss is 0.11535295844078064\n",
      "epoch: 1 step: 1148, loss is 0.05924947187304497\n",
      "epoch: 1 step: 1149, loss is 0.15369123220443726\n",
      "epoch: 1 step: 1150, loss is 0.007480946835130453\n",
      "epoch: 1 step: 1151, loss is 0.11355740576982498\n",
      "epoch: 1 step: 1152, loss is 0.11742227524518967\n",
      "epoch: 1 step: 1153, loss is 0.4262745976448059\n",
      "epoch: 1 step: 1154, loss is 0.05694147199392319\n",
      "epoch: 1 step: 1155, loss is 0.1616324782371521\n",
      "epoch: 1 step: 1156, loss is 0.16125108301639557\n",
      "epoch: 1 step: 1157, loss is 0.2425241470336914\n",
      "epoch: 1 step: 1158, loss is 0.09331894665956497\n",
      "epoch: 1 step: 1159, loss is 0.04767801612615585\n",
      "epoch: 1 step: 1160, loss is 0.4201628565788269\n",
      "epoch: 1 step: 1161, loss is 0.2999265193939209\n",
      "epoch: 1 step: 1162, loss is 0.45136725902557373\n",
      "epoch: 1 step: 1163, loss is 0.28055712580680847\n",
      "epoch: 1 step: 1164, loss is 0.17141948640346527\n",
      "epoch: 1 step: 1165, loss is 0.43604809045791626\n",
      "epoch: 1 step: 1166, loss is 0.17746742069721222\n",
      "epoch: 1 step: 1167, loss is 0.04492059722542763\n",
      "epoch: 1 step: 1168, loss is 0.4076792001724243\n",
      "epoch: 1 step: 1169, loss is 0.10480264574289322\n",
      "epoch: 1 step: 1170, loss is 0.40540260076522827\n",
      "epoch: 1 step: 1171, loss is 0.17907306551933289\n",
      "epoch: 1 step: 1172, loss is 0.3107132613658905\n",
      "epoch: 1 step: 1173, loss is 0.3615524470806122\n",
      "epoch: 1 step: 1174, loss is 0.13113512098789215\n",
      "epoch: 1 step: 1175, loss is 0.14869920909404755\n",
      "epoch: 1 step: 1176, loss is 0.05193613842129707\n",
      "epoch: 1 step: 1177, loss is 0.2399982213973999\n",
      "epoch: 1 step: 1178, loss is 0.22908583283424377\n",
      "epoch: 1 step: 1179, loss is 0.15731024742126465\n",
      "epoch: 1 step: 1180, loss is 0.19178687036037445\n",
      "epoch: 1 step: 1181, loss is 0.0861329510807991\n",
      "epoch: 1 step: 1182, loss is 0.4090081751346588\n",
      "epoch: 1 step: 1183, loss is 0.5095085501670837\n",
      "epoch: 1 step: 1184, loss is 0.10775049775838852\n",
      "epoch: 1 step: 1185, loss is 0.28576332330703735\n",
      "epoch: 1 step: 1186, loss is 0.16278305649757385\n",
      "epoch: 1 step: 1187, loss is 0.22381529211997986\n",
      "epoch: 1 step: 1188, loss is 0.1584273874759674\n",
      "epoch: 1 step: 1189, loss is 0.1574714630842209\n",
      "epoch: 1 step: 1190, loss is 0.1399872601032257\n",
      "epoch: 1 step: 1191, loss is 0.297325998544693\n",
      "epoch: 1 step: 1192, loss is 0.04040839523077011\n",
      "epoch: 1 step: 1193, loss is 0.0974922627210617\n",
      "epoch: 1 step: 1194, loss is 0.21084362268447876\n",
      "epoch: 1 step: 1195, loss is 0.1362631767988205\n",
      "epoch: 1 step: 1196, loss is 0.05329391732811928\n",
      "epoch: 1 step: 1197, loss is 0.14630049467086792\n",
      "epoch: 1 step: 1198, loss is 0.16281476616859436\n",
      "epoch: 1 step: 1199, loss is 0.16078080236911774\n",
      "epoch: 1 step: 1200, loss is 0.19904324412345886\n",
      "epoch: 1 step: 1201, loss is 0.09832882136106491\n",
      "epoch: 1 step: 1202, loss is 0.11219719052314758\n",
      "epoch: 1 step: 1203, loss is 0.27004942297935486\n",
      "epoch: 1 step: 1204, loss is 0.18900208175182343\n",
      "epoch: 1 step: 1205, loss is 0.04235192388296127\n",
      "epoch: 1 step: 1206, loss is 0.3006862699985504\n",
      "epoch: 1 step: 1207, loss is 0.04189891740679741\n",
      "epoch: 1 step: 1208, loss is 0.33300116658210754\n",
      "epoch: 1 step: 1209, loss is 0.2893555164337158\n",
      "epoch: 1 step: 1210, loss is 0.10693356394767761\n",
      "epoch: 1 step: 1211, loss is 0.10670832544565201\n",
      "epoch: 1 step: 1212, loss is 0.020431293174624443\n",
      "epoch: 1 step: 1213, loss is 0.1026727631688118\n",
      "epoch: 1 step: 1214, loss is 0.061060234904289246\n",
      "epoch: 1 step: 1215, loss is 0.05338151380419731\n",
      "epoch: 1 step: 1216, loss is 0.11570966988801956\n",
      "epoch: 1 step: 1217, loss is 0.29761093854904175\n",
      "epoch: 1 step: 1218, loss is 0.3704529404640198\n",
      "epoch: 1 step: 1219, loss is 0.13758505880832672\n",
      "epoch: 1 step: 1220, loss is 0.16150446236133575\n",
      "epoch: 1 step: 1221, loss is 0.218064084649086\n",
      "epoch: 1 step: 1222, loss is 0.5772188305854797\n",
      "epoch: 1 step: 1223, loss is 0.057993896305561066\n",
      "epoch: 1 step: 1224, loss is 0.3305352032184601\n",
      "epoch: 1 step: 1225, loss is 0.19890040159225464\n",
      "epoch: 1 step: 1226, loss is 0.02219421975314617\n",
      "epoch: 1 step: 1227, loss is 0.17800463736057281\n",
      "epoch: 1 step: 1228, loss is 0.24291370809078217\n",
      "epoch: 1 step: 1229, loss is 0.36020350456237793\n",
      "epoch: 1 step: 1230, loss is 0.3310858905315399\n",
      "epoch: 1 step: 1231, loss is 0.0912960097193718\n",
      "epoch: 1 step: 1232, loss is 0.15918327867984772\n",
      "epoch: 1 step: 1233, loss is 0.13957242667675018\n",
      "epoch: 1 step: 1234, loss is 0.27740055322647095\n",
      "epoch: 1 step: 1235, loss is 0.027705110609531403\n",
      "epoch: 1 step: 1236, loss is 0.046760864555835724\n",
      "epoch: 1 step: 1237, loss is 0.02018953673541546\n",
      "epoch: 1 step: 1238, loss is 0.40734872221946716\n",
      "epoch: 1 step: 1239, loss is 0.44388777017593384\n",
      "epoch: 1 step: 1240, loss is 0.35253435373306274\n",
      "epoch: 1 step: 1241, loss is 0.1976671814918518\n",
      "epoch: 1 step: 1242, loss is 0.33899447321891785\n",
      "epoch: 1 step: 1243, loss is 0.12353713810443878\n",
      "epoch: 1 step: 1244, loss is 0.2264423966407776\n",
      "epoch: 1 step: 1245, loss is 0.2212197482585907\n",
      "epoch: 1 step: 1246, loss is 0.07968088239431381\n",
      "epoch: 1 step: 1247, loss is 0.5516790151596069\n",
      "epoch: 1 step: 1248, loss is 0.2511654496192932\n",
      "epoch: 1 step: 1249, loss is 0.18526363372802734\n",
      "epoch: 1 step: 1250, loss is 0.4088498651981354\n",
      "epoch: 1 step: 1251, loss is 0.26026442646980286\n",
      "epoch: 1 step: 1252, loss is 0.14090941846370697\n",
      "epoch: 1 step: 1253, loss is 0.14454983174800873\n",
      "epoch: 1 step: 1254, loss is 0.24088361859321594\n",
      "epoch: 1 step: 1255, loss is 0.2132541388273239\n",
      "epoch: 1 step: 1256, loss is 0.07933856546878815\n",
      "epoch: 1 step: 1257, loss is 0.10344009101390839\n",
      "epoch: 1 step: 1258, loss is 0.14261695742607117\n",
      "epoch: 1 step: 1259, loss is 0.16752277314662933\n",
      "epoch: 1 step: 1260, loss is 0.09662977606058121\n",
      "epoch: 1 step: 1261, loss is 0.08518093079328537\n",
      "epoch: 1 step: 1262, loss is 0.017387697473168373\n",
      "epoch: 1 step: 1263, loss is 0.03559136763215065\n",
      "epoch: 1 step: 1264, loss is 0.07189202308654785\n",
      "epoch: 1 step: 1265, loss is 0.08922924846410751\n",
      "epoch: 1 step: 1266, loss is 0.1278291940689087\n",
      "epoch: 1 step: 1267, loss is 0.04200395569205284\n",
      "epoch: 1 step: 1268, loss is 0.06649209558963776\n",
      "epoch: 1 step: 1269, loss is 0.09578236937522888\n",
      "epoch: 1 step: 1270, loss is 0.1242275983095169\n",
      "epoch: 1 step: 1271, loss is 0.18687653541564941\n",
      "epoch: 1 step: 1272, loss is 0.1436072736978531\n",
      "epoch: 1 step: 1273, loss is 0.15208443999290466\n",
      "epoch: 1 step: 1274, loss is 0.30288007855415344\n",
      "epoch: 1 step: 1275, loss is 0.05019563436508179\n",
      "epoch: 1 step: 1276, loss is 0.23487085103988647\n",
      "epoch: 1 step: 1277, loss is 0.3405669033527374\n",
      "epoch: 1 step: 1278, loss is 0.022026799619197845\n",
      "epoch: 1 step: 1279, loss is 0.021705325692892075\n",
      "epoch: 1 step: 1280, loss is 0.14253228902816772\n",
      "epoch: 1 step: 1281, loss is 0.035005006939172745\n",
      "epoch: 1 step: 1282, loss is 0.26024723052978516\n",
      "epoch: 1 step: 1283, loss is 0.06477900594472885\n",
      "epoch: 1 step: 1284, loss is 0.33108702301979065\n",
      "epoch: 1 step: 1285, loss is 0.15752312541007996\n",
      "epoch: 1 step: 1286, loss is 0.0323801264166832\n",
      "epoch: 1 step: 1287, loss is 0.07478917390108109\n",
      "epoch: 1 step: 1288, loss is 0.1871137171983719\n",
      "epoch: 1 step: 1289, loss is 0.04617565870285034\n",
      "epoch: 1 step: 1290, loss is 0.18046972155570984\n",
      "epoch: 1 step: 1291, loss is 0.11547162383794785\n",
      "epoch: 1 step: 1292, loss is 0.1448851227760315\n",
      "epoch: 1 step: 1293, loss is 0.2653290629386902\n",
      "epoch: 1 step: 1294, loss is 0.25573065876960754\n",
      "epoch: 1 step: 1295, loss is 0.050565388053655624\n",
      "epoch: 1 step: 1296, loss is 0.1344863772392273\n",
      "epoch: 1 step: 1297, loss is 0.09780494123697281\n",
      "epoch: 1 step: 1298, loss is 0.3401595950126648\n",
      "epoch: 1 step: 1299, loss is 0.18044008314609528\n",
      "epoch: 1 step: 1300, loss is 0.17276018857955933\n",
      "epoch: 1 step: 1301, loss is 0.08390743285417557\n",
      "epoch: 1 step: 1302, loss is 0.07879653573036194\n",
      "epoch: 1 step: 1303, loss is 0.24525833129882812\n",
      "epoch: 1 step: 1304, loss is 0.08891301602125168\n",
      "epoch: 1 step: 1305, loss is 0.15491142868995667\n",
      "epoch: 1 step: 1306, loss is 0.026000211015343666\n",
      "epoch: 1 step: 1307, loss is 0.06723912805318832\n",
      "epoch: 1 step: 1308, loss is 0.20443010330200195\n",
      "epoch: 1 step: 1309, loss is 0.334480345249176\n",
      "epoch: 1 step: 1310, loss is 0.1447264403104782\n",
      "epoch: 1 step: 1311, loss is 0.22552292048931122\n",
      "epoch: 1 step: 1312, loss is 0.247196763753891\n",
      "epoch: 1 step: 1313, loss is 0.17045366764068604\n",
      "epoch: 1 step: 1314, loss is 0.33653491735458374\n",
      "epoch: 1 step: 1315, loss is 0.01650363579392433\n",
      "epoch: 1 step: 1316, loss is 0.11680580675601959\n",
      "epoch: 1 step: 1317, loss is 0.14367762207984924\n",
      "epoch: 1 step: 1318, loss is 0.20104055106639862\n",
      "epoch: 1 step: 1319, loss is 0.010723584331572056\n",
      "epoch: 1 step: 1320, loss is 0.033254776149988174\n",
      "epoch: 1 step: 1321, loss is 0.3834153413772583\n",
      "epoch: 1 step: 1322, loss is 0.05456249788403511\n",
      "epoch: 1 step: 1323, loss is 0.22607964277267456\n",
      "epoch: 1 step: 1324, loss is 0.09129645675420761\n",
      "epoch: 1 step: 1325, loss is 0.21518072485923767\n",
      "epoch: 1 step: 1326, loss is 0.10838674753904343\n",
      "epoch: 1 step: 1327, loss is 0.1750846803188324\n",
      "epoch: 1 step: 1328, loss is 0.107598677277565\n",
      "epoch: 1 step: 1329, loss is 0.27149564027786255\n",
      "epoch: 1 step: 1330, loss is 0.04287169873714447\n",
      "epoch: 1 step: 1331, loss is 0.29183080792427063\n",
      "epoch: 1 step: 1332, loss is 0.09128374606370926\n",
      "epoch: 1 step: 1333, loss is 0.13729655742645264\n",
      "epoch: 1 step: 1334, loss is 0.3045862913131714\n",
      "epoch: 1 step: 1335, loss is 0.06146223470568657\n",
      "epoch: 1 step: 1336, loss is 0.18998433649539948\n",
      "epoch: 1 step: 1337, loss is 0.08009814471006393\n",
      "epoch: 1 step: 1338, loss is 0.17334124445915222\n",
      "epoch: 1 step: 1339, loss is 0.09953756630420685\n",
      "epoch: 1 step: 1340, loss is 0.18466739356517792\n",
      "epoch: 1 step: 1341, loss is 0.12915143370628357\n",
      "epoch: 1 step: 1342, loss is 0.2656649947166443\n",
      "epoch: 1 step: 1343, loss is 0.017869414761662483\n",
      "epoch: 1 step: 1344, loss is 0.2806490659713745\n",
      "epoch: 1 step: 1345, loss is 0.06230545789003372\n",
      "epoch: 1 step: 1346, loss is 0.17368283867835999\n",
      "epoch: 1 step: 1347, loss is 0.13885392248630524\n",
      "epoch: 1 step: 1348, loss is 0.19556760787963867\n",
      "epoch: 1 step: 1349, loss is 0.11865296959877014\n",
      "epoch: 1 step: 1350, loss is 0.07733272761106491\n",
      "epoch: 1 step: 1351, loss is 0.08026394993066788\n",
      "epoch: 1 step: 1352, loss is 0.06304445117712021\n",
      "epoch: 1 step: 1353, loss is 0.04847484454512596\n",
      "epoch: 1 step: 1354, loss is 0.14887596666812897\n",
      "epoch: 1 step: 1355, loss is 0.09780849516391754\n",
      "epoch: 1 step: 1356, loss is 0.020444311201572418\n",
      "epoch: 1 step: 1357, loss is 0.2315039187669754\n",
      "epoch: 1 step: 1358, loss is 0.12211804836988449\n",
      "epoch: 1 step: 1359, loss is 0.15595872700214386\n",
      "epoch: 1 step: 1360, loss is 0.28813663125038147\n",
      "epoch: 1 step: 1361, loss is 0.021648572757840157\n",
      "epoch: 1 step: 1362, loss is 0.07005134224891663\n",
      "epoch: 1 step: 1363, loss is 0.014206521213054657\n",
      "epoch: 1 step: 1364, loss is 0.23720315098762512\n",
      "epoch: 1 step: 1365, loss is 0.3795478641986847\n",
      "epoch: 1 step: 1366, loss is 0.15162146091461182\n",
      "epoch: 1 step: 1367, loss is 0.0960967168211937\n",
      "epoch: 1 step: 1368, loss is 0.11411775648593903\n",
      "epoch: 1 step: 1369, loss is 0.3044019639492035\n",
      "epoch: 1 step: 1370, loss is 0.08325263857841492\n",
      "epoch: 1 step: 1371, loss is 0.38291501998901367\n",
      "epoch: 1 step: 1372, loss is 0.1651882827281952\n",
      "epoch: 1 step: 1373, loss is 0.03661783039569855\n",
      "epoch: 1 step: 1374, loss is 0.26247936487197876\n",
      "epoch: 1 step: 1375, loss is 0.2665373682975769\n",
      "epoch: 1 step: 1376, loss is 0.03127318248152733\n",
      "epoch: 1 step: 1377, loss is 0.15196388959884644\n",
      "epoch: 1 step: 1378, loss is 0.3594052493572235\n",
      "epoch: 1 step: 1379, loss is 0.055838536471128464\n",
      "epoch: 1 step: 1380, loss is 0.22891183197498322\n",
      "epoch: 1 step: 1381, loss is 0.16372588276863098\n",
      "epoch: 1 step: 1382, loss is 0.1637405902147293\n",
      "epoch: 1 step: 1383, loss is 0.28639158606529236\n",
      "epoch: 1 step: 1384, loss is 0.14758387207984924\n",
      "epoch: 1 step: 1385, loss is 0.14066597819328308\n",
      "epoch: 1 step: 1386, loss is 0.14229144155979156\n",
      "epoch: 1 step: 1387, loss is 0.3627719581127167\n",
      "epoch: 1 step: 1388, loss is 0.3781856894493103\n",
      "epoch: 1 step: 1389, loss is 0.09286598116159439\n",
      "epoch: 1 step: 1390, loss is 0.11191980540752411\n",
      "epoch: 1 step: 1391, loss is 0.007646059617400169\n",
      "epoch: 1 step: 1392, loss is 0.11568186432123184\n",
      "epoch: 1 step: 1393, loss is 0.0441378653049469\n",
      "epoch: 1 step: 1394, loss is 0.025380130857229233\n",
      "epoch: 1 step: 1395, loss is 0.06731096655130386\n",
      "epoch: 1 step: 1396, loss is 0.15471240878105164\n",
      "epoch: 1 step: 1397, loss is 0.044622667133808136\n",
      "epoch: 1 step: 1398, loss is 0.14015355706214905\n",
      "epoch: 1 step: 1399, loss is 0.11216127127408981\n",
      "epoch: 1 step: 1400, loss is 0.15763817727565765\n",
      "epoch: 1 step: 1401, loss is 0.18925456702709198\n",
      "epoch: 1 step: 1402, loss is 0.1115993857383728\n",
      "epoch: 1 step: 1403, loss is 0.3183199465274811\n",
      "epoch: 1 step: 1404, loss is 0.1567346602678299\n",
      "epoch: 1 step: 1405, loss is 0.09557142108678818\n",
      "epoch: 1 step: 1406, loss is 0.06972454488277435\n",
      "epoch: 1 step: 1407, loss is 0.2727491557598114\n",
      "epoch: 1 step: 1408, loss is 0.32487037777900696\n",
      "epoch: 1 step: 1409, loss is 0.05925551801919937\n",
      "epoch: 1 step: 1410, loss is 0.10534018278121948\n",
      "epoch: 1 step: 1411, loss is 0.0841454416513443\n",
      "epoch: 1 step: 1412, loss is 0.14913246035575867\n",
      "epoch: 1 step: 1413, loss is 0.2884579598903656\n",
      "epoch: 1 step: 1414, loss is 0.019839130342006683\n",
      "epoch: 1 step: 1415, loss is 0.16627348959445953\n",
      "epoch: 1 step: 1416, loss is 0.24573005735874176\n",
      "epoch: 1 step: 1417, loss is 0.1176876425743103\n",
      "epoch: 1 step: 1418, loss is 0.3118191659450531\n",
      "epoch: 1 step: 1419, loss is 0.30137214064598083\n",
      "epoch: 1 step: 1420, loss is 0.05551013723015785\n",
      "epoch: 1 step: 1421, loss is 0.34868761897087097\n",
      "epoch: 1 step: 1422, loss is 0.13172763586044312\n",
      "epoch: 1 step: 1423, loss is 0.11039634048938751\n",
      "epoch: 1 step: 1424, loss is 0.15869784355163574\n",
      "epoch: 1 step: 1425, loss is 0.13721410930156708\n",
      "epoch: 1 step: 1426, loss is 0.12710708379745483\n",
      "epoch: 1 step: 1427, loss is 0.058506451547145844\n",
      "epoch: 1 step: 1428, loss is 0.01262274943292141\n",
      "epoch: 1 step: 1429, loss is 0.09512186050415039\n",
      "epoch: 1 step: 1430, loss is 0.07474005967378616\n",
      "epoch: 1 step: 1431, loss is 0.16669908165931702\n",
      "epoch: 1 step: 1432, loss is 0.16615155339241028\n",
      "epoch: 1 step: 1433, loss is 0.15724575519561768\n",
      "epoch: 1 step: 1434, loss is 0.17115220427513123\n",
      "epoch: 1 step: 1435, loss is 0.11769770830869675\n",
      "epoch: 1 step: 1436, loss is 0.08119037747383118\n",
      "epoch: 1 step: 1437, loss is 0.08397137373685837\n",
      "epoch: 1 step: 1438, loss is 0.13604865968227386\n",
      "epoch: 1 step: 1439, loss is 0.18968698382377625\n",
      "epoch: 1 step: 1440, loss is 0.03256119415163994\n",
      "epoch: 1 step: 1441, loss is 0.13051480054855347\n",
      "epoch: 1 step: 1442, loss is 0.21968653798103333\n",
      "epoch: 1 step: 1443, loss is 0.05981980636715889\n",
      "epoch: 1 step: 1444, loss is 0.21978548169136047\n",
      "epoch: 1 step: 1445, loss is 0.32734858989715576\n",
      "epoch: 1 step: 1446, loss is 0.02244948223233223\n",
      "epoch: 1 step: 1447, loss is 0.11288562417030334\n",
      "epoch: 1 step: 1448, loss is 0.13044409453868866\n",
      "epoch: 1 step: 1449, loss is 0.08703174442052841\n",
      "epoch: 1 step: 1450, loss is 0.01822676882147789\n",
      "epoch: 1 step: 1451, loss is 0.162393718957901\n",
      "epoch: 1 step: 1452, loss is 0.07256809622049332\n",
      "epoch: 1 step: 1453, loss is 0.016248982399702072\n",
      "epoch: 1 step: 1454, loss is 0.283985435962677\n",
      "epoch: 1 step: 1455, loss is 0.052616775035858154\n",
      "epoch: 1 step: 1456, loss is 0.0984019860625267\n",
      "epoch: 1 step: 1457, loss is 0.30123960971832275\n",
      "epoch: 1 step: 1458, loss is 0.11290337145328522\n",
      "epoch: 1 step: 1459, loss is 0.03667246922850609\n",
      "epoch: 1 step: 1460, loss is 0.01912746950984001\n",
      "epoch: 1 step: 1461, loss is 0.08322933316230774\n",
      "epoch: 1 step: 1462, loss is 0.40199461579322815\n",
      "epoch: 1 step: 1463, loss is 0.23990951478481293\n",
      "epoch: 1 step: 1464, loss is 0.1296786516904831\n",
      "epoch: 1 step: 1465, loss is 0.08280566334724426\n",
      "epoch: 1 step: 1466, loss is 0.2275448888540268\n",
      "epoch: 1 step: 1467, loss is 0.04623563960194588\n",
      "epoch: 1 step: 1468, loss is 0.1671811044216156\n",
      "epoch: 1 step: 1469, loss is 0.24732597172260284\n",
      "epoch: 1 step: 1470, loss is 0.14966721832752228\n",
      "epoch: 1 step: 1471, loss is 0.13142216205596924\n",
      "epoch: 1 step: 1472, loss is 0.033778734505176544\n",
      "epoch: 1 step: 1473, loss is 0.18954351544380188\n",
      "epoch: 1 step: 1474, loss is 0.2034999281167984\n",
      "epoch: 1 step: 1475, loss is 0.3062056005001068\n",
      "epoch: 1 step: 1476, loss is 0.0777161568403244\n",
      "epoch: 1 step: 1477, loss is 0.07112786173820496\n",
      "epoch: 1 step: 1478, loss is 0.3005315065383911\n",
      "epoch: 1 step: 1479, loss is 0.03856002911925316\n",
      "epoch: 1 step: 1480, loss is 0.06629875302314758\n",
      "epoch: 1 step: 1481, loss is 0.11070388555526733\n",
      "epoch: 1 step: 1482, loss is 0.054250266402959824\n",
      "epoch: 1 step: 1483, loss is 0.31171393394470215\n",
      "epoch: 1 step: 1484, loss is 0.034630827605724335\n",
      "epoch: 1 step: 1485, loss is 0.04256223514676094\n",
      "epoch: 1 step: 1486, loss is 0.08015162497758865\n",
      "epoch: 1 step: 1487, loss is 0.02809925377368927\n",
      "epoch: 1 step: 1488, loss is 0.051735371351242065\n",
      "epoch: 1 step: 1489, loss is 0.28564128279685974\n",
      "epoch: 1 step: 1490, loss is 0.20595143735408783\n",
      "epoch: 1 step: 1491, loss is 0.009812355041503906\n",
      "epoch: 1 step: 1492, loss is 0.09623860567808151\n",
      "epoch: 1 step: 1493, loss is 0.10782444477081299\n",
      "epoch: 1 step: 1494, loss is 0.10880815237760544\n",
      "epoch: 1 step: 1495, loss is 0.1265614628791809\n",
      "epoch: 1 step: 1496, loss is 0.06773938238620758\n",
      "epoch: 1 step: 1497, loss is 0.024912171065807343\n",
      "epoch: 1 step: 1498, loss is 0.027537010610103607\n",
      "epoch: 1 step: 1499, loss is 0.0995868667960167\n",
      "epoch: 1 step: 1500, loss is 0.09169046580791473\n",
      "epoch: 1 step: 1501, loss is 0.1420229971408844\n",
      "epoch: 1 step: 1502, loss is 0.04500694200396538\n",
      "epoch: 1 step: 1503, loss is 0.03917555510997772\n",
      "epoch: 1 step: 1504, loss is 0.3357229232788086\n",
      "epoch: 1 step: 1505, loss is 0.09932012110948563\n",
      "epoch: 1 step: 1506, loss is 0.14063186943531036\n",
      "epoch: 1 step: 1507, loss is 0.10680857300758362\n",
      "epoch: 1 step: 1508, loss is 0.1309116780757904\n",
      "epoch: 1 step: 1509, loss is 0.1884496808052063\n",
      "epoch: 1 step: 1510, loss is 0.0829559862613678\n",
      "epoch: 1 step: 1511, loss is 0.02993611805140972\n",
      "epoch: 1 step: 1512, loss is 0.15199171006679535\n",
      "epoch: 1 step: 1513, loss is 0.04266992583870888\n",
      "epoch: 1 step: 1514, loss is 0.07981709390878677\n",
      "epoch: 1 step: 1515, loss is 0.11398742347955704\n",
      "epoch: 1 step: 1516, loss is 0.051207561045885086\n",
      "epoch: 1 step: 1517, loss is 0.08992523699998856\n",
      "epoch: 1 step: 1518, loss is 0.1880282610654831\n",
      "epoch: 1 step: 1519, loss is 0.028990738093852997\n",
      "epoch: 1 step: 1520, loss is 0.17575104534626007\n",
      "epoch: 1 step: 1521, loss is 0.058087460696697235\n",
      "epoch: 1 step: 1522, loss is 0.16008368134498596\n",
      "epoch: 1 step: 1523, loss is 0.16240011155605316\n",
      "epoch: 1 step: 1524, loss is 0.03928511217236519\n",
      "epoch: 1 step: 1525, loss is 0.09782355278730392\n",
      "epoch: 1 step: 1526, loss is 0.22689709067344666\n",
      "epoch: 1 step: 1527, loss is 0.16750770807266235\n",
      "epoch: 1 step: 1528, loss is 0.21741750836372375\n",
      "epoch: 1 step: 1529, loss is 0.03293241187930107\n",
      "epoch: 1 step: 1530, loss is 0.42334598302841187\n",
      "epoch: 1 step: 1531, loss is 0.03152861073613167\n",
      "epoch: 1 step: 1532, loss is 0.19105707108974457\n",
      "epoch: 1 step: 1533, loss is 0.45058509707450867\n",
      "epoch: 1 step: 1534, loss is 0.10678832232952118\n",
      "epoch: 1 step: 1535, loss is 0.2654052972793579\n",
      "epoch: 1 step: 1536, loss is 0.08462543040513992\n",
      "epoch: 1 step: 1537, loss is 0.029355516657233238\n",
      "epoch: 1 step: 1538, loss is 0.1903490573167801\n",
      "epoch: 1 step: 1539, loss is 0.1703832745552063\n",
      "epoch: 1 step: 1540, loss is 0.03636084496974945\n",
      "epoch: 1 step: 1541, loss is 0.1037270724773407\n",
      "epoch: 1 step: 1542, loss is 0.05482109636068344\n",
      "epoch: 1 step: 1543, loss is 0.03058742918074131\n",
      "epoch: 1 step: 1544, loss is 0.023030903190374374\n",
      "epoch: 1 step: 1545, loss is 0.08158160001039505\n",
      "epoch: 1 step: 1546, loss is 0.19057044386863708\n",
      "epoch: 1 step: 1547, loss is 0.06549815833568573\n",
      "epoch: 1 step: 1548, loss is 0.16315799951553345\n",
      "epoch: 1 step: 1549, loss is 0.1597258746623993\n",
      "epoch: 1 step: 1550, loss is 0.09487120807170868\n",
      "epoch: 1 step: 1551, loss is 0.019381524994969368\n",
      "epoch: 1 step: 1552, loss is 0.2660248875617981\n",
      "epoch: 1 step: 1553, loss is 0.07975508272647858\n",
      "epoch: 1 step: 1554, loss is 0.1735074371099472\n",
      "epoch: 1 step: 1555, loss is 0.04467843100428581\n",
      "epoch: 1 step: 1556, loss is 0.07329335063695908\n",
      "epoch: 1 step: 1557, loss is 0.05853279307484627\n",
      "epoch: 1 step: 1558, loss is 0.31804853677749634\n",
      "epoch: 1 step: 1559, loss is 0.15426050126552582\n",
      "epoch: 1 step: 1560, loss is 0.08898688107728958\n",
      "epoch: 1 step: 1561, loss is 0.040153391659259796\n",
      "epoch: 1 step: 1562, loss is 0.05261186510324478\n",
      "epoch: 1 step: 1563, loss is 0.03515208140015602\n",
      "epoch: 1 step: 1564, loss is 0.07604371011257172\n",
      "epoch: 1 step: 1565, loss is 0.07010165601968765\n",
      "epoch: 1 step: 1566, loss is 0.1894744634628296\n",
      "epoch: 1 step: 1567, loss is 0.11719377338886261\n",
      "epoch: 1 step: 1568, loss is 0.013539507053792477\n",
      "epoch: 1 step: 1569, loss is 0.06335076689720154\n",
      "epoch: 1 step: 1570, loss is 0.317080020904541\n",
      "epoch: 1 step: 1571, loss is 0.5321597456932068\n",
      "epoch: 1 step: 1572, loss is 0.27896037697792053\n",
      "epoch: 1 step: 1573, loss is 0.013066397048532963\n",
      "epoch: 1 step: 1574, loss is 0.19164110720157623\n",
      "epoch: 1 step: 1575, loss is 0.05986211448907852\n",
      "epoch: 1 step: 1576, loss is 0.11472566425800323\n",
      "epoch: 1 step: 1577, loss is 0.0293479822576046\n",
      "epoch: 1 step: 1578, loss is 0.15066850185394287\n",
      "epoch: 1 step: 1579, loss is 0.4707428812980652\n",
      "epoch: 1 step: 1580, loss is 0.48046502470970154\n",
      "epoch: 1 step: 1581, loss is 0.143973708152771\n",
      "epoch: 1 step: 1582, loss is 0.1170392632484436\n",
      "epoch: 1 step: 1583, loss is 0.057287875562906265\n",
      "epoch: 1 step: 1584, loss is 0.05644196271896362\n",
      "epoch: 1 step: 1585, loss is 0.1466371864080429\n",
      "epoch: 1 step: 1586, loss is 0.1092011108994484\n",
      "epoch: 1 step: 1587, loss is 0.2173912674188614\n",
      "epoch: 1 step: 1588, loss is 0.04632928594946861\n",
      "epoch: 1 step: 1589, loss is 0.2195953130722046\n",
      "epoch: 1 step: 1590, loss is 0.26160237193107605\n",
      "epoch: 1 step: 1591, loss is 0.10517669469118118\n",
      "epoch: 1 step: 1592, loss is 0.1896972507238388\n",
      "epoch: 1 step: 1593, loss is 0.18199598789215088\n",
      "epoch: 1 step: 1594, loss is 0.11840949952602386\n",
      "epoch: 1 step: 1595, loss is 0.1263485699892044\n",
      "epoch: 1 step: 1596, loss is 0.1157456561923027\n",
      "epoch: 1 step: 1597, loss is 0.11975841969251633\n",
      "epoch: 1 step: 1598, loss is 0.05977525934576988\n",
      "epoch: 1 step: 1599, loss is 0.1024676114320755\n",
      "epoch: 1 step: 1600, loss is 0.14970439672470093\n",
      "epoch: 1 step: 1601, loss is 0.009680349379777908\n",
      "epoch: 1 step: 1602, loss is 0.0032256655395030975\n",
      "epoch: 1 step: 1603, loss is 0.08406119048595428\n",
      "epoch: 1 step: 1604, loss is 0.03392699360847473\n",
      "epoch: 1 step: 1605, loss is 0.25593626499176025\n",
      "epoch: 1 step: 1606, loss is 0.01055898517370224\n",
      "epoch: 1 step: 1607, loss is 0.25339919328689575\n",
      "epoch: 1 step: 1608, loss is 0.24063171446323395\n",
      "epoch: 1 step: 1609, loss is 0.06999911367893219\n",
      "epoch: 1 step: 1610, loss is 0.07381823658943176\n",
      "epoch: 1 step: 1611, loss is 0.0861424058675766\n",
      "epoch: 1 step: 1612, loss is 0.05946897342801094\n",
      "epoch: 1 step: 1613, loss is 0.1920444816350937\n",
      "epoch: 1 step: 1614, loss is 0.18433472514152527\n",
      "epoch: 1 step: 1615, loss is 0.06491697579622269\n",
      "epoch: 1 step: 1616, loss is 0.043284591287374496\n",
      "epoch: 1 step: 1617, loss is 0.35413432121276855\n",
      "epoch: 1 step: 1618, loss is 0.10812229663133621\n",
      "epoch: 1 step: 1619, loss is 0.19480818510055542\n",
      "epoch: 1 step: 1620, loss is 0.006187906488776207\n",
      "epoch: 1 step: 1621, loss is 0.01765153557062149\n",
      "epoch: 1 step: 1622, loss is 0.010135618038475513\n",
      "epoch: 1 step: 1623, loss is 0.08260002732276917\n",
      "epoch: 1 step: 1624, loss is 0.12670938670635223\n",
      "epoch: 1 step: 1625, loss is 0.07419420778751373\n",
      "epoch: 1 step: 1626, loss is 0.023381777107715607\n",
      "epoch: 1 step: 1627, loss is 0.04100409150123596\n",
      "epoch: 1 step: 1628, loss is 0.2749912440776825\n",
      "epoch: 1 step: 1629, loss is 0.04121312126517296\n",
      "epoch: 1 step: 1630, loss is 0.07042660564184189\n",
      "epoch: 1 step: 1631, loss is 0.19728398323059082\n",
      "epoch: 1 step: 1632, loss is 0.2021716684103012\n",
      "epoch: 1 step: 1633, loss is 0.09919776022434235\n",
      "epoch: 1 step: 1634, loss is 0.10901035368442535\n",
      "epoch: 1 step: 1635, loss is 0.07771196961402893\n",
      "epoch: 1 step: 1636, loss is 0.04515424370765686\n",
      "epoch: 1 step: 1637, loss is 0.023220283910632133\n",
      "epoch: 1 step: 1638, loss is 0.10302481800317764\n",
      "epoch: 1 step: 1639, loss is 0.17020465433597565\n",
      "epoch: 1 step: 1640, loss is 0.018640071153640747\n",
      "epoch: 1 step: 1641, loss is 0.010138267651200294\n",
      "epoch: 1 step: 1642, loss is 0.032130442559719086\n",
      "epoch: 1 step: 1643, loss is 0.03670429810881615\n",
      "epoch: 1 step: 1644, loss is 0.24919988214969635\n",
      "epoch: 1 step: 1645, loss is 0.08300817012786865\n",
      "epoch: 1 step: 1646, loss is 0.03137704357504845\n",
      "epoch: 1 step: 1647, loss is 0.07951287925243378\n",
      "epoch: 1 step: 1648, loss is 0.16948412358760834\n",
      "epoch: 1 step: 1649, loss is 0.17639531195163727\n",
      "epoch: 1 step: 1650, loss is 0.2873268723487854\n",
      "epoch: 1 step: 1651, loss is 0.0045014056377112865\n",
      "epoch: 1 step: 1652, loss is 0.032324157655239105\n",
      "epoch: 1 step: 1653, loss is 0.14419493079185486\n",
      "epoch: 1 step: 1654, loss is 0.19020144641399384\n",
      "epoch: 1 step: 1655, loss is 0.21826773881912231\n",
      "epoch: 1 step: 1656, loss is 0.157558411359787\n",
      "epoch: 1 step: 1657, loss is 0.20337240397930145\n",
      "epoch: 1 step: 1658, loss is 0.014995457604527473\n",
      "epoch: 1 step: 1659, loss is 0.006280329078435898\n",
      "epoch: 1 step: 1660, loss is 0.026127401739358902\n",
      "epoch: 1 step: 1661, loss is 0.11685748398303986\n",
      "epoch: 1 step: 1662, loss is 0.11172836273908615\n",
      "epoch: 1 step: 1663, loss is 0.005955938715487719\n",
      "epoch: 1 step: 1664, loss is 0.03669792041182518\n",
      "epoch: 1 step: 1665, loss is 0.017572492361068726\n",
      "epoch: 1 step: 1666, loss is 0.11986802518367767\n",
      "epoch: 1 step: 1667, loss is 0.2615775167942047\n",
      "epoch: 1 step: 1668, loss is 0.04726390540599823\n",
      "epoch: 1 step: 1669, loss is 0.0873393639922142\n",
      "epoch: 1 step: 1670, loss is 0.05194631218910217\n",
      "epoch: 1 step: 1671, loss is 0.10715442895889282\n",
      "epoch: 1 step: 1672, loss is 0.02163304202258587\n",
      "epoch: 1 step: 1673, loss is 0.08778871595859528\n",
      "epoch: 1 step: 1674, loss is 0.09686069190502167\n",
      "epoch: 1 step: 1675, loss is 0.23320084810256958\n",
      "epoch: 1 step: 1676, loss is 0.015333164483308792\n",
      "epoch: 1 step: 1677, loss is 0.026290733367204666\n",
      "epoch: 1 step: 1678, loss is 0.1783480942249298\n",
      "epoch: 1 step: 1679, loss is 0.039993517100811005\n",
      "epoch: 1 step: 1680, loss is 0.3227250576019287\n",
      "epoch: 1 step: 1681, loss is 0.1178450882434845\n",
      "epoch: 1 step: 1682, loss is 0.06667125225067139\n",
      "epoch: 1 step: 1683, loss is 0.3577834665775299\n",
      "epoch: 1 step: 1684, loss is 0.05298585817217827\n",
      "epoch: 1 step: 1685, loss is 0.04263964295387268\n",
      "epoch: 1 step: 1686, loss is 0.08393905311822891\n",
      "epoch: 1 step: 1687, loss is 0.03770827129483223\n",
      "epoch: 1 step: 1688, loss is 0.126679465174675\n",
      "epoch: 1 step: 1689, loss is 0.21393990516662598\n",
      "epoch: 1 step: 1690, loss is 0.025865038856863976\n",
      "epoch: 1 step: 1691, loss is 0.1748206466436386\n",
      "epoch: 1 step: 1692, loss is 0.19075852632522583\n",
      "epoch: 1 step: 1693, loss is 0.00697797816246748\n",
      "epoch: 1 step: 1694, loss is 0.02390814572572708\n",
      "epoch: 1 step: 1695, loss is 0.17639461159706116\n",
      "epoch: 1 step: 1696, loss is 0.17993591725826263\n",
      "epoch: 1 step: 1697, loss is 0.017917465418577194\n",
      "epoch: 1 step: 1698, loss is 0.19168443977832794\n",
      "epoch: 1 step: 1699, loss is 0.1740713268518448\n",
      "epoch: 1 step: 1700, loss is 0.10355433821678162\n",
      "epoch: 1 step: 1701, loss is 0.12794345617294312\n",
      "epoch: 1 step: 1702, loss is 0.028407782316207886\n",
      "epoch: 1 step: 1703, loss is 0.010972781106829643\n",
      "epoch: 1 step: 1704, loss is 0.06399969756603241\n",
      "epoch: 1 step: 1705, loss is 0.15095989406108856\n",
      "epoch: 1 step: 1706, loss is 0.19682186841964722\n",
      "epoch: 1 step: 1707, loss is 0.024211885407567024\n",
      "epoch: 1 step: 1708, loss is 0.015185867436230183\n",
      "epoch: 1 step: 1709, loss is 0.02331761084496975\n",
      "epoch: 1 step: 1710, loss is 0.08504103869199753\n",
      "epoch: 1 step: 1711, loss is 0.15122446417808533\n",
      "epoch: 1 step: 1712, loss is 0.14698153734207153\n",
      "epoch: 1 step: 1713, loss is 0.10642465949058533\n",
      "epoch: 1 step: 1714, loss is 0.08998370915651321\n",
      "epoch: 1 step: 1715, loss is 0.02887820638716221\n",
      "epoch: 1 step: 1716, loss is 0.04456381872296333\n",
      "epoch: 1 step: 1717, loss is 0.040609754621982574\n",
      "epoch: 1 step: 1718, loss is 0.10487153381109238\n",
      "epoch: 1 step: 1719, loss is 0.23272845149040222\n",
      "epoch: 1 step: 1720, loss is 0.10388026386499405\n",
      "epoch: 1 step: 1721, loss is 0.03560731187462807\n",
      "epoch: 1 step: 1722, loss is 0.07271445542573929\n",
      "epoch: 1 step: 1723, loss is 0.2131439447402954\n",
      "epoch: 1 step: 1724, loss is 0.09859416633844376\n",
      "epoch: 1 step: 1725, loss is 0.13372640311717987\n",
      "epoch: 1 step: 1726, loss is 0.13622422516345978\n",
      "epoch: 1 step: 1727, loss is 0.14730952680110931\n",
      "epoch: 1 step: 1728, loss is 0.15172035992145538\n",
      "epoch: 1 step: 1729, loss is 0.03573593497276306\n",
      "epoch: 1 step: 1730, loss is 0.05167143791913986\n",
      "epoch: 1 step: 1731, loss is 0.05791127681732178\n",
      "epoch: 1 step: 1732, loss is 0.06589759886264801\n",
      "epoch: 1 step: 1733, loss is 0.19832473993301392\n",
      "epoch: 1 step: 1734, loss is 0.0534474141895771\n",
      "epoch: 1 step: 1735, loss is 0.023401843383908272\n",
      "epoch: 1 step: 1736, loss is 0.3132644593715668\n",
      "epoch: 1 step: 1737, loss is 0.15328069031238556\n",
      "epoch: 1 step: 1738, loss is 0.09472872316837311\n",
      "epoch: 1 step: 1739, loss is 0.012757502496242523\n",
      "epoch: 1 step: 1740, loss is 0.04439021274447441\n",
      "epoch: 1 step: 1741, loss is 0.2799699008464813\n",
      "epoch: 1 step: 1742, loss is 0.1195380911231041\n",
      "epoch: 1 step: 1743, loss is 0.0235286857932806\n",
      "epoch: 1 step: 1744, loss is 0.07327263802289963\n",
      "epoch: 1 step: 1745, loss is 0.15326181054115295\n",
      "epoch: 1 step: 1746, loss is 0.07020759582519531\n",
      "epoch: 1 step: 1747, loss is 0.029277140274643898\n",
      "epoch: 1 step: 1748, loss is 0.12996886670589447\n",
      "epoch: 1 step: 1749, loss is 0.03797323629260063\n",
      "epoch: 1 step: 1750, loss is 0.24120694398880005\n",
      "epoch: 1 step: 1751, loss is 0.16477760672569275\n",
      "epoch: 1 step: 1752, loss is 0.008995826356112957\n",
      "epoch: 1 step: 1753, loss is 0.0022143269889056683\n",
      "epoch: 1 step: 1754, loss is 0.0374605767428875\n",
      "epoch: 1 step: 1755, loss is 0.016157792881131172\n",
      "epoch: 1 step: 1756, loss is 0.06167484074831009\n",
      "epoch: 1 step: 1757, loss is 0.02522246539592743\n",
      "epoch: 1 step: 1758, loss is 0.12257594615221024\n",
      "epoch: 1 step: 1759, loss is 0.1658763289451599\n",
      "epoch: 1 step: 1760, loss is 0.07819708436727524\n",
      "epoch: 1 step: 1761, loss is 0.2091839611530304\n",
      "epoch: 1 step: 1762, loss is 0.013030287809669971\n",
      "epoch: 1 step: 1763, loss is 0.024561569094657898\n",
      "epoch: 1 step: 1764, loss is 0.029526859521865845\n",
      "epoch: 1 step: 1765, loss is 0.06020370498299599\n",
      "epoch: 1 step: 1766, loss is 0.25130537152290344\n",
      "epoch: 1 step: 1767, loss is 0.18634545803070068\n",
      "epoch: 1 step: 1768, loss is 0.016348782926797867\n",
      "epoch: 1 step: 1769, loss is 0.03819437697529793\n",
      "epoch: 1 step: 1770, loss is 0.01036469079554081\n",
      "epoch: 1 step: 1771, loss is 0.29609450697898865\n",
      "epoch: 1 step: 1772, loss is 0.09506826102733612\n",
      "epoch: 1 step: 1773, loss is 0.09893357008695602\n",
      "epoch: 1 step: 1774, loss is 0.03729486092925072\n",
      "epoch: 1 step: 1775, loss is 0.14279481768608093\n",
      "epoch: 1 step: 1776, loss is 0.008837654255330563\n",
      "epoch: 1 step: 1777, loss is 0.4163597822189331\n",
      "epoch: 1 step: 1778, loss is 0.1800176501274109\n",
      "epoch: 1 step: 1779, loss is 0.12098149210214615\n",
      "epoch: 1 step: 1780, loss is 0.06149214506149292\n",
      "epoch: 1 step: 1781, loss is 0.1597117781639099\n",
      "epoch: 1 step: 1782, loss is 0.007317832205444574\n",
      "epoch: 1 step: 1783, loss is 0.13989150524139404\n",
      "epoch: 1 step: 1784, loss is 0.12062214314937592\n",
      "epoch: 1 step: 1785, loss is 0.25991833209991455\n",
      "epoch: 1 step: 1786, loss is 0.24051082134246826\n",
      "epoch: 1 step: 1787, loss is 0.06980090588331223\n",
      "epoch: 1 step: 1788, loss is 0.036386825144290924\n",
      "epoch: 1 step: 1789, loss is 0.09023801982402802\n",
      "epoch: 1 step: 1790, loss is 0.10086572170257568\n",
      "epoch: 1 step: 1791, loss is 0.07027631253004074\n",
      "epoch: 1 step: 1792, loss is 0.11332875490188599\n",
      "epoch: 1 step: 1793, loss is 0.02084488235414028\n",
      "epoch: 1 step: 1794, loss is 0.22389455139636993\n",
      "epoch: 1 step: 1795, loss is 0.15863004326820374\n",
      "epoch: 1 step: 1796, loss is 0.33633318543434143\n",
      "epoch: 1 step: 1797, loss is 0.17474201321601868\n",
      "epoch: 1 step: 1798, loss is 0.0677448958158493\n",
      "epoch: 1 step: 1799, loss is 0.09149705618619919\n",
      "epoch: 1 step: 1800, loss is 0.022572416812181473\n",
      "epoch: 1 step: 1801, loss is 0.07384712994098663\n",
      "epoch: 1 step: 1802, loss is 0.04378578066825867\n",
      "epoch: 1 step: 1803, loss is 0.12258048355579376\n",
      "epoch: 1 step: 1804, loss is 0.005082411225885153\n",
      "epoch: 1 step: 1805, loss is 0.1706918627023697\n",
      "epoch: 1 step: 1806, loss is 0.08034513890743256\n",
      "epoch: 1 step: 1807, loss is 0.032610129565000534\n",
      "epoch: 1 step: 1808, loss is 0.10939038544893265\n",
      "epoch: 1 step: 1809, loss is 0.21715806424617767\n",
      "epoch: 1 step: 1810, loss is 0.09572029858827591\n",
      "epoch: 1 step: 1811, loss is 0.010994608514010906\n",
      "epoch: 1 step: 1812, loss is 0.03476639464497566\n",
      "epoch: 1 step: 1813, loss is 0.12153241038322449\n",
      "epoch: 1 step: 1814, loss is 0.24987740814685822\n",
      "epoch: 1 step: 1815, loss is 0.025467226281762123\n",
      "epoch: 1 step: 1816, loss is 0.027064379304647446\n",
      "epoch: 1 step: 1817, loss is 0.05355729162693024\n",
      "epoch: 1 step: 1818, loss is 0.053858719766139984\n",
      "epoch: 1 step: 1819, loss is 0.016118476167321205\n",
      "epoch: 1 step: 1820, loss is 0.01584492437541485\n",
      "epoch: 1 step: 1821, loss is 0.01974506303668022\n",
      "epoch: 1 step: 1822, loss is 0.014318529516458511\n",
      "epoch: 1 step: 1823, loss is 0.020993584766983986\n",
      "epoch: 1 step: 1824, loss is 0.22469432651996613\n",
      "epoch: 1 step: 1825, loss is 0.21560294926166534\n",
      "epoch: 1 step: 1826, loss is 0.11687082052230835\n",
      "epoch: 1 step: 1827, loss is 0.049509063363075256\n",
      "epoch: 1 step: 1828, loss is 0.20384417474269867\n",
      "epoch: 1 step: 1829, loss is 0.01850859262049198\n",
      "epoch: 1 step: 1830, loss is 0.010973386466503143\n",
      "epoch: 1 step: 1831, loss is 0.5142335891723633\n",
      "epoch: 1 step: 1832, loss is 0.15320615470409393\n",
      "epoch: 1 step: 1833, loss is 0.08061570674180984\n",
      "epoch: 1 step: 1834, loss is 0.17657729983329773\n",
      "epoch: 1 step: 1835, loss is 0.012823916040360928\n",
      "epoch: 1 step: 1836, loss is 0.09071686863899231\n",
      "epoch: 1 step: 1837, loss is 0.00762743316590786\n",
      "epoch: 1 step: 1838, loss is 0.07088598608970642\n",
      "epoch: 1 step: 1839, loss is 0.11622336506843567\n",
      "epoch: 1 step: 1840, loss is 0.007891236804425716\n",
      "epoch: 1 step: 1841, loss is 0.09513407945632935\n",
      "epoch: 1 step: 1842, loss is 0.07056160271167755\n",
      "epoch: 1 step: 1843, loss is 0.06583525240421295\n",
      "epoch: 1 step: 1844, loss is 0.13608507812023163\n",
      "epoch: 1 step: 1845, loss is 0.06530042737722397\n",
      "epoch: 1 step: 1846, loss is 0.13912883400917053\n",
      "epoch: 1 step: 1847, loss is 0.1221570298075676\n",
      "epoch: 1 step: 1848, loss is 0.20089827477931976\n",
      "epoch: 1 step: 1849, loss is 0.13445301353931427\n",
      "epoch: 1 step: 1850, loss is 0.0732249990105629\n",
      "epoch: 1 step: 1851, loss is 0.1445864588022232\n",
      "epoch: 1 step: 1852, loss is 0.16091488301753998\n",
      "epoch: 1 step: 1853, loss is 0.06156601384282112\n",
      "epoch: 1 step: 1854, loss is 0.1383674591779709\n",
      "epoch: 1 step: 1855, loss is 0.04497673735022545\n",
      "epoch: 1 step: 1856, loss is 0.01691579818725586\n",
      "epoch: 1 step: 1857, loss is 0.0049318005330860615\n",
      "epoch: 1 step: 1858, loss is 0.07620235532522202\n",
      "epoch: 1 step: 1859, loss is 0.011849875561892986\n",
      "epoch: 1 step: 1860, loss is 0.2393789142370224\n",
      "epoch: 1 step: 1861, loss is 0.14525331556797028\n",
      "epoch: 1 step: 1862, loss is 0.3059084415435791\n",
      "epoch: 1 step: 1863, loss is 0.07464639097452164\n",
      "epoch: 1 step: 1864, loss is 0.05840903893113136\n",
      "epoch: 1 step: 1865, loss is 0.0559401772916317\n",
      "epoch: 1 step: 1866, loss is 0.11975172162055969\n",
      "epoch: 1 step: 1867, loss is 0.20270612835884094\n",
      "epoch: 1 step: 1868, loss is 0.09999925643205643\n",
      "epoch: 1 step: 1869, loss is 0.017699068412184715\n",
      "epoch: 1 step: 1870, loss is 0.14716173708438873\n",
      "epoch: 1 step: 1871, loss is 0.24898532032966614\n",
      "epoch: 1 step: 1872, loss is 0.31793534755706787\n",
      "epoch: 1 step: 1873, loss is 0.12554098665714264\n",
      "epoch: 1 step: 1874, loss is 0.02867145836353302\n",
      "epoch: 1 step: 1875, loss is 0.03315611556172371\n",
      "============== Starting Testing ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:54.196.247 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:54.196.247 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:54.196.247 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:54.196.247 [mindspore\\dataset\\core\\validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(29352:29224,MainProcess):2024-04-27-10:50:54.196.247 [mindspore\\dataset\\core\\validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Accuracy:{'Accuracy': 0.9680488782051282} ==============\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import argparse\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor\n",
    "from mindspore.train import Model\n",
    "import mindspore.ops.operations as P\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "\n",
    "# import mindspore.dataset.transforms.vision.c_transforms as CV\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "\n",
    "# from mindspore.dataset.transforms.vision import Inter\n",
    "from mindspore.dataset.vision import Inter\n",
    "\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "\n",
    "\n",
    "# import mindspore.dataset.vision.py_transforms as py_vision\n",
    "# from mindspore.dataset.transforms import c_transforms\n",
    "\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    \"\"\" create dataset for train or test\n",
    "    Args:\n",
    "        data_path: Data path\n",
    "        batch_size: The number of data records in each group\n",
    "        repeat_size: The number of replicated data records\n",
    "        num_parallel_workers: The number of parallel workers\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # define operation parameters\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # define map operations\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)  # Resize images to (32, 32)\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml) # normalize images\n",
    "    rescale_op = CV.Rescale(rescale, shift) # rescale images\n",
    "    hwc2chw_op = CV.HWC2CHW() # change shape from (height, width, channel) to (channel, height, width) to fit network.\n",
    "    type_cast_op = C.TypeCast(mstype.int32) # change data type of label to int32 to fit network\n",
    "\n",
    "    # apply map operations on images\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"label\", operations=type_cast_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=resize_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rescale_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rescale_nml_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=hwc2chw_op, num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    # apply DatasetOps\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)  # 10000 as in LeNet train script\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds\n",
    "\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"Conv layer weight initial.\"\"\"\n",
    "    weight = weight_variable()\n",
    "    return nn.Conv2d(in_channels, out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                     weight_init=weight, has_bias=False, pad_mode=\"valid\")\n",
    "\n",
    "\n",
    "def fc_with_initialize(input_channels, out_channels):\n",
    "    \"\"\"Fc layer weight initial.\"\"\"\n",
    "    weight = weight_variable()\n",
    "    bias = weight_variable()\n",
    "    return nn.Dense(input_channels, out_channels, weight, bias)\n",
    "\n",
    "\n",
    "def weight_variable():\n",
    "    \"\"\"Weight initial.\"\"\"\n",
    "    return TruncatedNormal(0.02)\n",
    "\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"Lenet network structure.\"\"\"\n",
    "    # define the operator required\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.batch_size = 32\n",
    "        self.conv1 = conv(1, 6, 5)\n",
    "        self.conv2 = conv(6, 16, 5)\n",
    "        self.fc1 = fc_with_initialize(16 * 5 * 5, 120)\n",
    "        self.fc2 = fc_with_initialize(120, 84)\n",
    "        self.fc3 = fc_with_initialize(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.reshape = P.Reshape()\n",
    "\n",
    "    # use the preceding operators to construct networks\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.reshape(x, (self.batch_size, -1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_net(args, model, epoch_size, mnist_path, repeat_size, ckpoint_cb):\n",
    "    \"\"\"Define the training method.\"\"\"\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    # load training dataset\n",
    "    ds_train = create_dataset(os.path.join(mnist_path, \"train\"), 32, repeat_size)\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor()], dataset_sink_mode=False)\n",
    "\n",
    "\n",
    "def test_net(args, network, model, mnist_path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    print(\"============== Starting Testing ==============\")\n",
    "    # load the saved model for evaluation\n",
    "    param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
    "    # load parameter to the network\n",
    "    load_param_into_net(network, param_dict)\n",
    "    # load testing dataset\n",
    "    ds_eval = create_dataset(os.path.join(mnist_path, \"test\"))\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
    "    print(\"============== Accuracy:{} ==============\".format(acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='MindSpore LeNet Example')\n",
    "    parser.add_argument('--device_target', type=str, default=\"Ascend\", choices=['Ascend', 'GPU', 'CPU'],\n",
    "                        help='device where the code will be implemented (default: Ascend)')\n",
    "    \n",
    "    # for jupyter notebook\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(args=['--device_target', 'CPU'])\n",
    "\n",
    "    # context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target, mem_Reuse=False)\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)\n",
    "\n",
    "    # learning rate setting\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "    epoch_size = 1\n",
    "    mnist_path = \"./MNIST\"\n",
    "    # define the loss function\n",
    "    # net_loss = SoftmaxCrossEntropyWithLogits(is_grad=False, sparse=True, reduction='mean')\n",
    "    net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "\n",
    "    repeat_size = epoch_size\n",
    "    # create the network\n",
    "    network = LeNet5()\n",
    "    # define the optimizer\n",
    "    net_opt = nn.Momentum(network.trainable_params(), lr, momentum)\n",
    "    config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "    # save the network model and parameters for subsequence fine-tuning\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)\n",
    "    # group layers into an object with training and evaluation features\n",
    "    model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "\n",
    "    train_net(args, model, epoch_size, mnist_path, repeat_size, ckpoint_cb)\n",
    "    test_net(args, network, model, mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
